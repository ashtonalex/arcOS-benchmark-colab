{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQmG_saT2Xy9"
   },
   "source": [
    "# arcOS Benchmark - Causal QA with GNN + LLM\n",
    "\n",
    "**Phase 1: Environment & Data Foundation**\n",
    "\n",
    "This notebook implements the complete arcOS benchmark pipeline:\n",
    "- Graph Neural Network structural reasoning over knowledge graphs\n",
    "- LLM text generation with graph-guided prompts\n",
    "- Evaluation on RoG-WebQSP question answering dataset\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 or better)\n",
    "- Google Drive mounted for checkpointing\n",
    "- ~10GB free space on Drive\n",
    "\n",
    "**Architecture:**\n",
    "- Dataset: RoG-WebQSP (4,706 QA pairs with Freebase subgraphs)\n",
    "- Graph DB: NetworkX in-memory\n",
    "- GNN: Graph Attention Network (GATv2)\n",
    "- LLM: OpenRouter API (Claude 3.5 Sonnet)\n",
    "- Verbalization: Hard prompts (text-based, not soft embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Cloning Repository\n",
    "Update notebook with latest updates from GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing directory if it exists, then clone fresh\n",
    "!rm -rf /content/arcOS-benchmark-colab\n",
    "!git clone https://github.com/ashtonalex/arcOS-benchmark-colab /content/arcOS-benchmark-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD04nv-w2XzA"
   },
   "source": [
    "## Cell 1: Environment Setup\n",
    "\n",
    "Install dependencies and verify GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1Y0UZMb2XzA",
    "outputId": "e09f5891-3133-4beb-df1b-e7b584e58482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: ENVIRONMENT PATH VERIFICATION\n",
      "======================================================================\n",
      "Current kernel executable: /usr/bin/python3\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "Site packages: /content\n",
      "✓ UV available: uv 0.9.26\n",
      "\n",
      "======================================================================\n",
      "STEP 2: PACKAGE INSTALLATION\n",
      "======================================================================\n",
      "Installing packages using UV with --python /usr/bin/python3\n",
      "\n",
      "Installing PyTorch with CUDA 11.8 support...\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m\n",
      "\n",
      "Installing additional dependencies...\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 26ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "STEP 3: INSTALLATION VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "Verifying installed packages:\n",
      "\n",
      "✓ torch        v2.9.0+cpu   \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/torch\n",
      "\n",
      "✓ datasets     v4.0.0       \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/datasets\n",
      "\n",
      "✓ networkx     v3.6.1       \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/networkx\n",
      "\n",
      "✓ tqdm         v4.67.3      \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/tqdm\n",
      "\n",
      "✓ faiss        v1.13.2      \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/faiss\n",
      "\n",
      "======================================================================\n",
      "STEP 4: GPU VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "GPU available: False ✗\n",
      "⚠ Warning: No GPU detected.\n",
      "  Go to: Runtime -> Change runtime type -> Select T4 GPU\n",
      "\n",
      "======================================================================\n",
      "ENVIRONMENT SETUP SUMMARY\n",
      "======================================================================\n",
      "Package manager: UV\n",
      "Python executable: /usr/bin/python3\n",
      "All packages verified: ✓ YES\n",
      "GPU available: ✗ NO\n",
      "======================================================================\n",
      "\n",
      "✓ Environment setup complete with full parity!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP WITH UV PACKAGE MANAGER\n",
    "# Ensures absolute environment parity between kernel and installed packages\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab UV workaround: Clear broken constraint files\n",
    "os.environ[\"UV_CONSTRAINT\"] = \"\"\n",
    "os.environ[\"UV_BUILD_CONSTRAINT\"] = \"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: ENVIRONMENT PATH VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Capture current Python executable\n",
    "current_python = sys.executable\n",
    "print(f\"Current kernel executable: {current_python}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Site packages: {sys.path[0] if sys.path else 'N/A'}\")\n",
    "\n",
    "# Check if uv is available\n",
    "def check_uv_available():\n",
    "    \"\"\"Check if uv is installed and accessible.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['uv', '--version'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=5\n",
    "        )\n",
    "        return result.returncode == 0\n",
    "    except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "        return False\n",
    "\n",
    "uv_available = check_uv_available()\n",
    "\n",
    "if not uv_available:\n",
    "    print(\"\\n⚠ UV not found. Installing uv package manager...\")\n",
    "    %pip install -q uv\n",
    "    uv_available = check_uv_available()\n",
    "\n",
    "if uv_available:\n",
    "    # Get uv version\n",
    "    uv_version = subprocess.run(\n",
    "        ['uv', '--version'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    ).stdout.strip()\n",
    "    print(f\"✓ UV available: {uv_version}\")\n",
    "else:\n",
    "    print(\"✗ UV installation failed. Will fall back to pip.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PACKAGE INSTALLATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define packages to install\n",
    "packages = [\n",
    "    \"datasets\",\n",
    "    \"networkx\",\n",
    "    \"tqdm\",\n",
    "    \"faiss-gpu-cu12\" # Added faiss-gpu\n",
    "]\n",
    "\n",
    "# PyTorch with CUDA support\n",
    "torch_packages = \"torch torchvision torchaudio\"\n",
    "torch_index = \"https://download.pytorch.org/whl/cu118\"\n",
    "\n",
    "if uv_available:\n",
    "    print(f\"Installing packages using UV with --python {current_python}\\n\")\n",
    "\n",
    "    # Install PyTorch with CUDA\n",
    "    print(\"Installing PyTorch with CUDA 11.8 support...\")\n",
    "    !uv pip install --python {current_python} {torch_packages} --index-url {torch_index}\n",
    "\n",
    "    # Install other packages\n",
    "    print(\"\\nInstalling additional dependencies...\")\n",
    "    for package in packages:\n",
    "        !uv pip install --python {current_python} {package}\n",
    "else:\n",
    "    print(\"Falling back to standard pip installation\\n\")\n",
    "\n",
    "    # Install PyTorch with CUDA\n",
    "    print(\"Installing PyTorch with CUDA 11.8 support...\")\n",
    "    %pip install -q {torch_packages} --index-url {torch_index}\n",
    "\n",
    "    # Install other packages\n",
    "    print(\"\\nInstalling additional dependencies...\")\n",
    "    %pip install -q {' '.join(packages)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: INSTALLATION VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify installed packages are in the correct location\n",
    "def verify_package_location(package_name):\n",
    "    \"\"\"Verify package is installed in current kernel's site-packages.\"\"\"\n",
    "    try:\n",
    "        module = __import__(package_name)\n",
    "        module_path = Path(module.__file__).parent\n",
    "\n",
    "        # Check if module is in one of sys.path locations\n",
    "        in_sys_path = any(str(module_path).startswith(p) for p in sys.path if p)\n",
    "\n",
    "        # Get version if available\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "\n",
    "        return {\n",
    "            'installed': True,\n",
    "            'version': version,\n",
    "            'location': str(module_path),\n",
    "            'in_sys_path': in_sys_path\n",
    "        }\n",
    "    except ImportError:\n",
    "        return {'installed': False}\n",
    "\n",
    "# Verify key packages\n",
    "verification_packages = ['torch', 'datasets', 'networkx', 'tqdm', 'faiss'] # Added faiss for verification\n",
    "print(\"\\nVerifying installed packages:\\n\")\n",
    "\n",
    "all_verified = True\n",
    "for pkg in verification_packages:\n",
    "    info = verify_package_location(pkg)\n",
    "    if info['installed']:\n",
    "        status = \"✓\" if info['in_sys_path'] else \"⚠\"\n",
    "        print(f\"{status} {pkg:12s} v{info['version']:12s}\")\n",
    "        print(f\"  Location: {info['location']}\")\n",
    "        if not info['in_sys_path']:\n",
    "            print(f\"  WARNING: Not in sys.path!\")\n",
    "            all_verified = False\n",
    "    else:\n",
    "        print(f\"✗ {pkg:12s} NOT INSTALLED\")\n",
    "        all_verified = False\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: GPU VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"\\nGPU available: {gpu_available} {'✓' if gpu_available else '✗'}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠ Warning: No GPU detected.\")\n",
    "    print(\"  Go to: Runtime -> Change runtime type -> Select T4 GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENVIRONMENT SETUP SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Package manager: {'UV' if uv_available else 'pip'}\")\n",
    "print(f\"Python executable: {current_python}\")\n",
    "print(f\"All packages verified: {'✓ YES' if all_verified else '✗ NO'}\")\n",
    "print(f\"GPU available: {'✓ YES' if gpu_available else '✗ NO'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not all_verified:\n",
    "    print(\"\\n⚠ WARNING: Some packages failed verification. Check output above.\")\n",
    "else:\n",
    "    print(\"\\n✓ Environment setup complete with full parity!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv2OR5W52XzC"
   },
   "source": [
    "## Cell 2: Import Modules\n",
    "\n",
    "Import arcOS benchmark modules from `src/` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqjQCB9b2XzC",
    "outputId": "4fcce3cf-0afe-4693-8394-755c927c26ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ ERROR: Could not find src/ directory\n",
      "Current directory: /content\n",
      "\n",
      "Searched in the following locations:\n",
      "  1. /content (exists: True, has src/: False)\n",
      "  2. / (exists: True, has src/: False)\n",
      "  3. /content/arcOS-benchmark-colab (exists: False, has src/: False)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "src/ not found - check project structure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1915351515.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  {i}. {root} (error: {e})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src/ not found - check project structure\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Add to Python path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: src/ not found - check project structure",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Auto-detect project root (works with Colab, local Jupyter, and VSCode)\n",
    "# Try multiple strategies to find the project root\n",
    "possible_roots = []\n",
    "\n",
    "# Strategy 1: Check if we're in the notebooks/ folder\n",
    "if Path.cwd().name == \"notebooks\":\n",
    "    possible_roots.append(Path.cwd().parent)\n",
    "\n",
    "# Strategy 2: Check current directory\n",
    "possible_roots.append(Path.cwd())\n",
    "\n",
    "# Strategy 3: Check parent directory (for when running from notebooks/)\n",
    "possible_roots.append(Path.cwd().parent)\n",
    "\n",
    "# Strategy 4: Check if notebook file path is available (Jupyter/VSCode)\n",
    "try:\n",
    "    # Try to get the notebook's directory from IPython\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    if ipython and hasattr(ipython, 'user_ns'):\n",
    "        # In Jupyter/VSCode, __file__ might be available\n",
    "        notebook_path = ipython.user_ns.get('__vsc_ipynb_file__')\n",
    "        if notebook_path:\n",
    "            possible_roots.append(Path(notebook_path).parent.parent)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Strategy 5: Colab-specific paths\n",
    "possible_roots.extend([\n",
    "    Path(\"arcOS-benchmark-colab\"),\n",
    "    Path(\"/content/arcOS-benchmark-colab\"),\n",
    "    Path(\"/content\"),\n",
    "])\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_roots = []\n",
    "for root in possible_roots:\n",
    "    root_abs = root.resolve()\n",
    "    if root_abs not in seen:\n",
    "        seen.add(root_abs)\n",
    "        unique_roots.append(root)\n",
    "\n",
    "# Search for project root\n",
    "project_root = None\n",
    "for root in unique_roots:\n",
    "    try:\n",
    "        src_path = root / \"src\"\n",
    "        if src_path.exists() and (src_path / \"config.py\").exists():\n",
    "            project_root = root.resolve()\n",
    "            break\n",
    "    except (OSError, PermissionError):\n",
    "        continue\n",
    "\n",
    "if project_root is None:\n",
    "    print(\"⚠ ERROR: Could not find src/ directory\")\n",
    "    print(f\"Current directory: {Path.cwd()}\")\n",
    "    print(f\"\\nSearched in the following locations:\")\n",
    "    for i, root in enumerate(unique_roots, 1):\n",
    "        try:\n",
    "            abs_path = root.resolve()\n",
    "            exists = root.exists()\n",
    "            src_exists = (root / \"src\").exists() if exists else False\n",
    "            print(f\"  {i}. {abs_path} (exists: {exists}, has src/: {src_exists})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {i}. {root} (error: {e})\")\n",
    "    raise ImportError(\"src/ not found - check project structure\")\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"✓ Found project root: {project_root}\")\n",
    "print(f\"  Current directory: {Path.cwd()}\")\n",
    "\n",
    "# Import modules\n",
    "print(\"\\nImporting modules...\")\n",
    "try:\n",
    "    from src.config import BenchmarkConfig\n",
    "    from src.utils.seeds import set_seeds\n",
    "    from src.utils.checkpoints import (\n",
    "        ensure_drive_mounted,\n",
    "        checkpoint_exists,\n",
    "        save_checkpoint,\n",
    "        load_checkpoint,\n",
    "        create_checkpoint_dirs,\n",
    "    )\n",
    "    from src.data.dataset_loader import RoGWebQSPLoader\n",
    "    from src.data.graph_builder import GraphBuilder\n",
    "    print(\"✓ All imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import failed: {e}\")\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    print(f\"sys.path: {sys.path[:3]}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbv4KpD12XzD"
   },
   "source": [
    "## Cell 3: Configuration\n",
    "\n",
    "Initialize benchmark configuration with hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbf7sF7A2XzE",
    "outputId": "efae91da-0ceb-431d-91bf-e432271a18b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "arcOS Benchmark Configuration\n",
      "============================================================\n",
      "Seed: 42 (deterministic=True)\n",
      "Dataset: rmanluo/RoG-webqsp\n",
      "Drive root: /content/drive/MyDrive/arcOS_benchmark\n",
      "Checkpoint dir: /content/drive/MyDrive/arcOS_benchmark/checkpoints\n",
      "Results dir: /content/drive/MyDrive/arcOS_benchmark/results\n",
      "\n",
      "--- Retrieval ---\n",
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Top-K entities: 15\n",
      "PCST budget: 50\n",
      "PCST local budget: 300\n",
      "PCST edge cost: 0.1\n",
      "\n",
      "--- GNN ---\n",
      "Hidden dim: 256\n",
      "Num layers: 3\n",
      "Num heads: 4\n",
      "Pooling: attention\n",
      "\n",
      "--- LLM ---\n",
      "Model: anthropic/claude-3.5-sonnet\n",
      "Provider: openrouter\n",
      "Temperature: 0.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize configuration\n",
    "config = BenchmarkConfig(\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    drive_root=\"/content/drive/MyDrive/arcOS_benchmark\",\n",
    ")\n",
    "\n",
    "# Print configuration summary\n",
    "config.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrmwQcua2XzG"
   },
   "source": [
    "## Cell 4: Seed Initialization\n",
    "\n",
    "Set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuvkZMYQ2XzH",
    "outputId": "9d0a2e8e-e4a2-48d7-e59c-deae42bcaaa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seeds set to 42 (deterministic=True)\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds(config.seed, config.deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgEi3ieQ2XzH"
   },
   "source": [
    "## Cell 5: Google Drive Setup\n",
    "\n",
    "Mount Google Drive and create checkpoint/results directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjnHwHNG2XzI",
    "outputId": "f562ecf5-3f47-4e21-f4b2-f15035b69400"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ensure_drive_mounted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-156369524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive_mounted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_drive_mounted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdrive_mounted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create checkpoint and results directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ensure_drive_mounted' is not defined"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "drive_mounted = ensure_drive_mounted()\n",
    "\n",
    "if drive_mounted:\n",
    "    # Create checkpoint and results directories\n",
    "    create_checkpoint_dirs(config.checkpoint_dir, config.results_dir)\n",
    "else:\n",
    "    print(\"⚠ Warning: Drive not mounted. Checkpointing will not work.\")\n",
    "    print(\"  Continuing with local /content/ storage (temporary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_PN8Rjb2XzI"
   },
   "source": [
    "## Cell 6: Dataset Loading\n",
    "\n",
    "Load RoG-WebQSP dataset from HuggingFace with Drive caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "HKOysb1o2XzJ",
    "outputId": "c1726f96-dc6e-424a-a92a-5b3847d29ffa"
   },
   "outputs": [],
   "source": [
    "# Initialize dataset loader\n",
    "cache_dir = config.checkpoint_dir / \"huggingface_cache\"\n",
    "loader = RoGWebQSPLoader(cache_dir=cache_dir)\n",
    "\n",
    "# Check for cached dataset\n",
    "dataset_checkpoint_path = config.get_checkpoint_path(\"dataset.pkl\")\n",
    "\n",
    "dataset = None # Initialize dataset to None\n",
    "\n",
    "if checkpoint_exists(dataset_checkpoint_path):\n",
    "    print(\"Loading dataset from checkpoint...\")\n",
    "    try:\n",
    "        dataset = load_checkpoint(dataset_checkpoint_path, format=\"pickle\")\n",
    "        print(\"✓ Dataset loaded from checkpoint.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠ Warning: Failed to load dataset from checkpoint due to missing files: {e}\")\n",
    "        print(\"  Falling back to downloading dataset from HuggingFace...\")\n",
    "        # If loading fails, proceed to download\n",
    "        pass # dataset remains None, so the next block will execute\n",
    "\n",
    "if dataset is None: # If dataset was not loaded successfully or checkpoint didn't exist\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    dataset = loader.load(dataset_name=config.dataset_name)\n",
    "    save_checkpoint(dataset, dataset_checkpoint_path, format=\"pickle\")\n",
    "    print(\"✓ Dataset downloaded and saved to checkpoint.\")\n",
    "\n",
    "# Slice dataset to desired sizes\n",
    "dataset = loader.slice_dataset(\n",
    "    dataset,\n",
    "    train_size=900,\n",
    "    val_size=90,\n",
    "    test_size=None  # Keep all test examples\n",
    ")\n",
    "\n",
    "# Inspect dataset schema\n",
    "loader.inspect_schema(dataset, num_examples=1)\n",
    "\n",
    "# Compute statistics\n",
    "loader.compute_statistics(dataset)\n",
    "\n",
    "# Validate split counts (updated for sliced dataset)\n",
    "split_valid = loader.validate_split_counts(\n",
    "    dataset,\n",
    "    expected_train=900,\n",
    "    expected_val=90,\n",
    "    expected_test=1628,  # Keep original test size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tpm668x2XzJ"
   },
   "source": [
    "## Cell 7: Graph Construction\n",
    "\n",
    "Build NetworkX graphs from dataset triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "UBzceOT52XzJ",
    "outputId": "fe7f4170-7b15-4b64-b334-10df45175d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GraphBuilder initialized (directed=True)\n",
      "Building unified graph from training split...\n",
      "Building unified graph from dataset...\n",
      "  Processed 500/2826 examples...\n",
      "  Processed 1000/2826 examples...\n",
      "  Processed 1500/2826 examples...\n",
      "  Processed 2000/2826 examples...\n",
      "  Processed 2500/2826 examples...\n",
      "✓ Unified graph built from 2826 examples\n",
      "  Total triples processed: 11951780\n",
      "  Unique nodes: 1023103\n",
      "  Unique edges: 2889277\n",
      "✓ Checkpoint saved: /content/drive/MyDrive/arcOS_benchmark/checkpoints/unified_graph.pkl (pickle)\n",
      "\n",
      "============================================================\n",
      "Unified Training Graph Information\n",
      "============================================================\n",
      "Nodes: 1023103\n",
      "Edges: 2889277\n",
      "Directed: True\n",
      "Density: 0.000003\n",
      "Weakly connected: True\n",
      "\n",
      "Relation Statistics:\n",
      "Unique relations: 5622\n",
      "Top 10 relations:\n",
      "  - common.topic.notable_types: 135525\n",
      "  - common.topic.notable_for: 70887\n",
      "  - location.location.containedby: 67294\n",
      "  - freebase.valuenotation.is_reviewed: 67203\n",
      "  - people.person.profession: 44770\n",
      "  - location.statistical_region.population: 42123\n",
      "  - common.topic.article: 41886\n",
      "  - people.person.gender: 40946\n",
      "  - common.topic.webpage: 38099\n",
      "  - common.webpage.topic: 38029\n",
      "\n",
      "Degree Statistics:\n",
      "Average in-degree: 2.82\n",
      "Average out-degree: 2.82\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Graph Size Validation\n",
      "============================================================\n",
      "Nodes: 1023103 ✓\n",
      "Edges: 2889277 ✓\n",
      "\n",
      "✓ Graph meets size requirements\n",
      "============================================================\n",
      "\n",
      "Building sample per-example graph...\n",
      "\n",
      "============================================================\n",
      "Sample Per-Example Graph Information\n",
      "============================================================\n",
      "Nodes: 1723\n",
      "Edges: 8286\n",
      "Directed: True\n",
      "Density: 0.002793\n",
      "Weakly connected: True\n",
      "\n",
      "Relation Statistics:\n",
      "Unique relations: 320\n",
      "Top 10 relations:\n",
      "  - freebase.valuenotation.is_reviewed: 898\n",
      "  - broadcast.content.artist: 771\n",
      "  - music.artist.genre: 742\n",
      "  - broadcast.artist.content: 631\n",
      "  - people.person.profession: 614\n",
      "  - common.topic.notable_types: 395\n",
      "  - people.person.gender: 208\n",
      "  - people.person.nationality: 179\n",
      "  - broadcast.content.genre: 136\n",
      "  - common.topic.notable_for: 135\n",
      "\n",
      "Degree Statistics:\n",
      "Average in-degree: 4.81\n",
      "Average out-degree: 4.81\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize graph builder\n",
    "graph_builder = GraphBuilder(directed=config.graph_directed)\n",
    "\n",
    "# Check for cached unified graph\n",
    "unified_graph_path = config.get_checkpoint_path(\"unified_graph.pkl\")\n",
    "\n",
    "if checkpoint_exists(unified_graph_path):\n",
    "    print(\"Loading unified graph from checkpoint...\")\n",
    "    unified_graph = load_checkpoint(unified_graph_path, format=\"pickle\")\n",
    "else:\n",
    "    print(\"Building unified graph from training split...\")\n",
    "    unified_graph = graph_builder.build_unified_graph(dataset[\"train\"])\n",
    "    save_checkpoint(unified_graph, unified_graph_path, format=\"pickle\")\n",
    "\n",
    "# Print graph statistics\n",
    "graph_builder.print_graph_info(unified_graph, name=\"Unified Training Graph\")\n",
    "\n",
    "# Validate graph size\n",
    "graph_valid = graph_builder.validate_graph_size(\n",
    "    unified_graph,\n",
    "    min_nodes=config.unified_graph_min_nodes,\n",
    "    min_edges=config.unified_graph_min_edges,\n",
    ")\n",
    "\n",
    "# Build sample per-example graph for demonstration\n",
    "print(\"\\nBuilding sample per-example graph...\")\n",
    "sample_example = dataset[\"train\"][0]\n",
    "sample_graph = graph_builder.build_from_triples(\n",
    "    sample_example[\"graph\"],\n",
    "    graph_id=sample_example[\"id\"]\n",
    ")\n",
    "graph_builder.print_graph_info(sample_graph, name=\"Sample Per-Example Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pHieT482XzJ"
   },
   "source": [
    "## Cell 8: Phase 1 Validation\n",
    "\n",
    "Automated validation of all Phase 1 success criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEm6SVWK2XzK",
    "outputId": "3a4f0cd1-aaec-4e94-8118-be8d0aa20264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Phase 1 Success Criteria Validation\n",
      "============================================================\n",
      "✓ Checkpoint saved: /content/drive/MyDrive/arcOS_benchmark/checkpoints/test_roundtrip.pkl (pickle)\n",
      "✓ Checkpoint loaded: /content/drive/MyDrive/arcOS_benchmark/checkpoints/test_roundtrip.pkl (pickle)\n",
      "\n",
      "Validation Results:\n",
      "  ✗ GPU Available\n",
      "  ✓ All Imports Successful\n",
      "  ✗ Dataset Splits Valid\n",
      "  ✓ Unified Graph Size Valid\n",
      "  ✓ Checkpoint Round-Trip\n",
      "\n",
      "============================================================\n",
      "✗ PHASE 1 INCOMPLETE - Some criteria failed\n",
      "\n",
      "Please review failed criteria above\n",
      "============================================================\n",
      "\n",
      "Phase 1 Summary:\n",
      "  Dataset: rmanluo/RoG-webqsp\n",
      "  Training examples: 2826\n",
      "  Validation examples: 246\n",
      "  Test examples: 1628\n",
      "  Unified graph nodes: 1023103\n",
      "  Unified graph edges: 2889277\n",
      "  Checkpoints saved to: /content/drive/MyDrive/arcOS_benchmark/checkpoints\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 1 Success Criteria Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect validation results\n",
    "validation_results = {\n",
    "    \"GPU Available\": torch.cuda.is_available(),\n",
    "    \"All Imports Successful\": True,  # If we got here, imports worked\n",
    "    \"Dataset Splits Valid\": split_valid,\n",
    "    \"Unified Graph Size Valid\": graph_valid,\n",
    "}\n",
    "\n",
    "# Test checkpoint round-trip\n",
    "test_checkpoint_path = config.get_checkpoint_path(\"test_roundtrip.pkl\")\n",
    "test_data = {\"test\": \"round-trip\", \"value\": 42}\n",
    "try:\n",
    "    save_checkpoint(test_data, test_checkpoint_path, format=\"pickle\")\n",
    "    loaded_data = load_checkpoint(test_checkpoint_path, format=\"pickle\")\n",
    "    checkpoint_roundtrip_ok = (loaded_data == test_data)\n",
    "    validation_results[\"Checkpoint Round-Trip\"] = checkpoint_roundtrip_ok\n",
    "except Exception as e:\n",
    "    print(f\"Checkpoint round-trip failed: {e}\")\n",
    "    validation_results[\"Checkpoint Round-Trip\"] = False\n",
    "\n",
    "# Print results\n",
    "print(\"\\nValidation Results:\")\n",
    "all_passed = True\n",
    "for criterion, passed in validation_results.items():\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"  {status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_passed:\n",
    "    print(\"✓ PHASE 1 COMPLETE - All criteria passed!\")\n",
    "    print(\"\\nReady to proceed to Phase 2: Retrieval Pipeline\")\n",
    "else:\n",
    "    print(\"✗ PHASE 1 INCOMPLETE - Some criteria failed\")\n",
    "    print(\"\\nPlease review failed criteria above\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nPhase 1 Summary:\")\n",
    "print(f\"  Dataset: {config.dataset_name}\")\n",
    "print(f\"  Training examples: {len(dataset['train'])}\")\n",
    "print(f\"  Validation examples: {len(dataset['validation'])}\")\n",
    "print(f\"  Test examples: {len(dataset['test'])}\")\n",
    "print(f\"  Unified graph nodes: {unified_graph.number_of_nodes()}\")\n",
    "print(f\"  Unified graph edges: {unified_graph.number_of_edges()}\")\n",
    "print(f\"  Checkpoints saved to: {config.checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN_Yhru52XzK"
   },
   "source": [
    "## Cell 9: Build Retrieval Pipeline\n",
    "\n",
    "Initialize retrieval components (embeddings, FAISS index, PCST solver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 153ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 80ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpcst-fast\u001b[0m\u001b[2m==1.0.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybind11\u001b[0m\u001b[2m==3.0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install pcst-fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954,
     "referenced_widgets": [
      "3b6e2a07271c470682ed57cd9d4ec68c",
      "506ddfadd72c4909bb9a2114d4cbfffe",
      "041c254f9d784065ba72b6a1c896e2c3",
      "f3b2fcc49b6e499bb078f6cdd90a363f",
      "f59f4bccda2c44ae9d15c4747b581a20",
      "70b4723234ef4730b6e20a685ecf7a5a",
      "099dbc8612f94ba4933b3bdcc06d069b",
      "1979c36da93e4028bb631b21d6edd717",
      "c42782e59f3f42b6968ebd3b6cd26ba7",
      "51ea187fab3a4b57be40aef0109cd90f",
      "c683e6e0e61a45dfb892cc701c70eb48"
     ]
    },
    "id": "I48MWhc62XzL",
    "outputId": "a3da42c9-bd23-43d8-ac19-953603bb6553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 2: RETRIEVAL PIPELINE\n",
      "============================================================\n",
      "============================================================\n",
      "BUILDING RETRIEVAL PIPELINE\n",
      "============================================================\n",
      "\n",
      "[1/4] Initializing text embedder...\n",
      "⚠ CUDA not available, falling back to CPU for embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ed490bd2e845f0ac57fb9f36f5e616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23947b0fe9e4462588c481dea808fcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21c919703d74cfc8c9ad99d2e92c8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9ecf2372ae4a968cd6ea9ea80a9405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ae52d2efab45438068a63784a3f9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcafaeab23ae49eea0959a388b229a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6833e8281e4b45a7be13e5fe93af945d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3d5784d3604b18b994e7c5999696bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c3f6074ede4f35a6ef34505e2e74d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e4bba471034d408b06013f6bae85ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed54d72f8cc641aba46375980a37e6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbd81dc00f443039903360ea290f586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  - Device: cpu\n",
      "  - Embedding dimension: 384\n",
      "\n",
      "[2/4] Loading/computing entity embeddings...\n",
      "Computing entity embeddings (this may take several minutes)...\n",
      "Embedding 1023103 entities (with relation enrichment)...\n",
      "  Enriched 1022965/1023103 entities with relation context\n",
      "    sample: P!nk | album, area of activism, artist, award winner, awards won, canoodled, content, contributor\n",
      "    sample: Gender | properties, rdf-schema#domain, rdf-schema#range, subjects\n",
      "    sample: 1Club.FM: Power | article, artist, broadcast, content, genre, image, location, notable for\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16df1b08cee45e18fea889f5b78b1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-452431471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Build retriever (uses checkpoints if available)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m retriever = Retriever.build_from_checkpoint_or_new(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0munified_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munified_graph\u001b[0m  \u001b[0;31m# From Phase 1 Cell 7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/arcOS-benchmark-colab/src/retrieval/retriever.py\u001b[0m in \u001b[0;36mbuild_from_checkpoint_or_new\u001b[0;34m(cls, config, unified_graph)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Computing entity embeddings (this may take several minutes)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0mentity_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_graph_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munified_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_embeddings_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/arcOS-benchmark-colab/src/retrieval/embeddings.py\u001b[0m in \u001b[0;36membed_graph_entities\u001b[0;34m(self, G, batch_size)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_enriched_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         entity_embeddings = {\n",
      "\u001b[0;32m/content/arcOS-benchmark-colab/src/retrieval/embeddings.py\u001b[0m in \u001b[0;36membed_texts\u001b[0;34m(self, texts, batch_size, show_progress)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         embeddings = self.model.encode(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                     \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_kwarg_keys\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"forward_kwargs\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 }\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mtrans_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m         )\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    697\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             hidden_states = layer_module(\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: RETRIEVAL PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from src.retrieval import Retriever\n",
    "\n",
    "# Build retriever (uses checkpoints if available)\n",
    "retriever = Retriever.build_from_checkpoint_or_new(\n",
    "    config=config,\n",
    "    unified_graph=unified_graph  # From Phase 1 Cell 7\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Retrieval pipeline initialized\")\n",
    "print(f\"  - Entity embeddings: {len(retriever.entity_index)} entities\")\n",
    "print(f\"  - Top-K: {config.top_k_entities}\")\n",
    "print(f\"  - PCST budget: {config.pcst_budget} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VwowPg62XzL"
   },
   "source": [
    "## Cell 10: Retrieval Validation\n",
    "\n",
    "Test retrieval pipeline on 10 validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "HfRB6T_p2XzL",
    "outputId": "e92539b3-a96d-4956-f2f9-dc61faaa5fd0"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RETRIEVAL VALIDATION (10 examples)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use first 10 validation examples\n",
    "val_examples = list(dataset[\"validation\"].select(range(10)))\n",
    "\n",
    "hit_count = 0\n",
    "total_time_ms = 0\n",
    "subgraph_sizes = []\n",
    "\n",
    "for i, example in enumerate(val_examples):\n",
    "    question = example[\"question\"]\n",
    "    answer_entities = example.get(\"a_entity\", [])\n",
    "    if isinstance(answer_entities, str):\n",
    "        answer_entities = [answer_entities]\n",
    "\n",
    "    # Extract topic entities from dataset\n",
    "    q_entities = example.get(\"q_entity\", [])\n",
    "    if isinstance(q_entities, str):\n",
    "        q_entities = [q_entities]\n",
    "\n",
    "    # Retrieve subgraph (q_entity used as primary seed when available)\n",
    "    result = retriever.retrieve(question, q_entity=q_entities)\n",
    "\n",
    "    # Check if answer entity in subgraph\n",
    "    subgraph_nodes = set(result.subgraph.nodes())\n",
    "    hit = any(ans in subgraph_nodes for ans in answer_entities)\n",
    "\n",
    "    if hit:\n",
    "        hit_count += 1\n",
    "\n",
    "    total_time_ms += result.retrieval_time_ms\n",
    "    subgraph_sizes.append(result.num_nodes)\n",
    "\n",
    "    # Print example\n",
    "    print(f\"\\n[{i+1}/10] Q: {question[:60]}...\")\n",
    "    print(f\"  Topic entities (q_entity): {q_entities}\")\n",
    "    print(f\"  Answer entities: {answer_entities}\")\n",
    "    print(f\"  Seeds used: {result.seed_entities[:5]}{'...' if len(result.seed_entities) > 5 else ''}\")\n",
    "    print(f\"  Subgraph: {result.num_nodes} nodes, {result.num_edges} edges\")\n",
    "    print(f\"  Hit: {'✓' if hit else '✗'}\")\n",
    "    print(f\"  Time: {result.retrieval_time_ms:.1f}ms\")\n",
    "\n",
    "# Summary metrics\n",
    "hit_rate = hit_count / len(val_examples) * 100\n",
    "avg_time = total_time_ms / len(val_examples)\n",
    "avg_size = sum(subgraph_sizes) / len(subgraph_sizes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Hit rate: {hit_rate:.1f}% ({hit_count}/{len(val_examples)})\")\n",
    "print(f\"Avg retrieval time: {avg_time:.1f}ms\")\n",
    "print(f\"Avg subgraph size: {avg_size:.1f} nodes\")\n",
    "print(f\"Max subgraph size: {max(subgraph_sizes)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju6e-sE22XzM"
   },
   "source": [
    "## Cell 11: Phase 2 Success Criteria\n",
    "\n",
    "Validate Phase 2 completion criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEDJ25h_2XzM",
    "outputId": "57355918-2aa3-4a8e-a4ef-5a0bfd4f4020"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2 SUCCESS CRITERIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criterion 1: Retrieval speed < 1 second\n",
    "speed_pass = avg_time < 1000  # ms\n",
    "print(f\"[{'✓' if speed_pass else '✗'}] Retrieval completes in <1 second: {avg_time:.1f}ms\")\n",
    "\n",
    "# Criterion 2: Hit rate > 60%\n",
    "hit_pass = hit_rate >= 60.0\n",
    "print(f\"[{'✓' if hit_pass else '✗'}] Subgraph contains answer entity >60%: {hit_rate:.1f}%\")\n",
    "\n",
    "# Criterion 3: All subgraphs connected\n",
    "all_connected = all(\n",
    "    nx.is_weakly_connected(\n",
    "        retriever.retrieve(\n",
    "            example[\"question\"],\n",
    "            q_entity=example.get(\"q_entity\")\n",
    "        ).subgraph\n",
    "    )\n",
    "    for example in val_examples[:5]  # Check first 5\n",
    ")\n",
    "print(f\"[{'✓' if all_connected else '✗'}] All subgraphs are connected\")\n",
    "\n",
    "# Criterion 4: Subgraph size respects budget\n",
    "size_pass = max(subgraph_sizes) <= config.pcst_budget\n",
    "print(f\"[{'✓' if size_pass else '✗'}] Subgraph size ≤ budget ({max(subgraph_sizes)} ≤ {config.pcst_budget})\")\n",
    "\n",
    "# Overall pass\n",
    "all_pass = speed_pass and hit_pass and all_connected and size_pass\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_pass:\n",
    "    print(\"✓ PHASE 2 COMPLETE - All criteria met!\")\n",
    "    print(\"\\nReady to proceed to Phase 3: GNN Encoder\")\n",
    "else:\n",
    "    print(\"⚠ PHASE 2 INCOMPLETE - Review failed criteria above\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdPaAEYiab3A"
   },
   "source": [
    "# Phase 3: GNN Encoder\n",
    "## Cell 12: Build/Load GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8YgdaqamjdWl",
    "outputId": "15f3fbd3-01fd-45be-dff2-21fef43fcace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 88ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbTg--JZkzUx",
    "outputId": "42a8b6fa-888d-4dc5-bc5a-664231ac08d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing torch_geometric...\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 88ms\u001b[0m\u001b[0m\n",
      "✓ torch_geometric installed.\n"
     ]
    }
   ],
   "source": [
    "# Install torch_geometric and its dependencies\n",
    "print(\"Installing torch_geometric...\")\n",
    "!uv pip install torch_geometric torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.8.0+cu128.html\n",
    "print(\"✓ torch_geometric installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "kiad8bLiab3B",
    "outputId": "c1a86141-eb79-49d1-8de3-c4354834b1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building GNN Model...\n",
      "This will either:\n",
      "  1. Load pre-trained model from checkpoint, OR\n",
      "  2. Prepare data and train from scratch (~30 min)\n",
      "\n",
      "============================================================\n",
      "Building GNN Model\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Retriever' object has no attribute 'entity_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2760833626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Build GNN model (handles checkpoint loading or training automatically)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m gnn_model = GNNModel.build_from_checkpoint_or_train(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/arcOS-benchmark-colab/src/gnn/model_wrapper.py\u001b[0m in \u001b[0;36mbuild_from_checkpoint_or_train\u001b[0;34m(cls, config, retriever, train_data, val_data, encoder_type, pooling_type)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Build converter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         converter = SubgraphConverter(\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mentity_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mrelation_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtext_embedder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_embedder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Retriever' object has no attribute 'entity_embeddings'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3: GNN Encoder\n",
    "# ============================================================\n",
    "\n",
    "print(\"Building GNN Model...\")\n",
    "print(\"This will either:\")\n",
    "print(\"  1. Load pre-trained model from checkpoint, OR\")\n",
    "print(\"  2. Prepare data and train from scratch (~30 min)\")\n",
    "print()\n",
    "\n",
    "from src.gnn import GNNModel\n",
    "\n",
    "# Build GNN model (handles checkpoint loading or training automatically)\n",
    "gnn_model = GNNModel.build_from_checkpoint_or_train(\n",
    "    config=config,\n",
    "    retriever=retriever,\n",
    "    train_data=dataset[\"train\"],\n",
    "    val_data=dataset[\"validation\"],\n",
    "    encoder_type=\"gatv2\",  # or \"graphsage\"\n",
    "    pooling_type=\"attention\",  # or \"mean\", \"max\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GNN Model Ready\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BbYDV0Zab3B"
   },
   "source": [
    "## Cell 13: Test GNN Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "J90pfuVTab3B",
    "outputId": "1d2ed4d0-9f08-4c45-a96c-07b57a64d634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GNN inference on example query...\n",
      "\n",
      "Question: Who is Justin Bieber's brother?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-45622549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Retrieve subgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mretrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Retrieved subgraph: {retrieved.num_nodes} nodes, {retrieved.num_edges} edges\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# Test GNN encoding on a single example\n",
    "print(\"Testing GNN inference on example query...\\n\")\n",
    "\n",
    "test_question = \"Who is Justin Bieber's brother?\"\n",
    "print(f\"Question: {test_question}\")\n",
    "\n",
    "# Retrieve subgraph\n",
    "retrieved = retriever.retrieve(test_question)\n",
    "print(f\"Retrieved subgraph: {retrieved.num_nodes} nodes, {retrieved.num_edges} edges\")\n",
    "\n",
    "# Encode with GNN\n",
    "gnn_output = gnn_model.encode(retrieved, test_question)\n",
    "\n",
    "print(f\"\\nGNN Output:\")\n",
    "print(f\"  Node embeddings shape: {gnn_output.node_embeddings.shape}\")\n",
    "print(f\"  Graph embedding shape: {gnn_output.graph_embedding.shape}\")\n",
    "print(f\"  Attention scores: {len(gnn_output.attention_scores)} nodes\")\n",
    "\n",
    "# Get top attention nodes\n",
    "top_nodes = gnn_model.get_top_attention_nodes(gnn_output, top_k=10)\n",
    "print(f\"\\nTop 10 nodes by attention score:\")\n",
    "for i, (node, score) in enumerate(top_nodes, 1):\n",
    "    print(f\"  {i}. {node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYEaOgU3ab3B"
   },
   "source": [
    "## Cell 14: Validate GNN Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lXQ_Uzgab3B"
   },
   "outputs": [],
   "source": [
    "# Load training history and display metrics\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_path = config.get_checkpoint_path(\"gnn_training_history.json\")\n",
    "with open(history_path, \"r\") as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 3 VALIDATION: GNN Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best metrics\n",
    "best_val_f1 = max(history[\"val_f1\"])\n",
    "best_val_loss = min(history[\"val_loss\"])\n",
    "final_val_f1 = history[\"val_f1\"][-1]\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Epochs trained: {len(history['train_loss'])}\")\n",
    "print(f\"  Best validation F1: {best_val_f1:.3f}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Final validation F1: {final_val_f1:.3f}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training and Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# F1 curve\n",
    "axes[1].plot(history[\"train_f1\"], label=\"Train F1\")\n",
    "axes[1].plot(history[\"val_f1\"], label=\"Val F1\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"F1 Score\")\n",
    "axes[1].set_title(\"Answer Node Prediction F1\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Success criteria check\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Success Criteria:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "criteria = [\n",
    "    (\"Validation F1 > 0.5\", best_val_f1 > 0.5, best_val_f1),\n",
    "    (\"Training completed < 30 min\", True, \"N/A\"),  # User observation\n",
    "    (\"No OOM errors\", True, \"N/A\"),  # User observation\n",
    "]\n",
    "\n",
    "for criterion, passed, value in criteria:\n",
    "    status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "    print(f\"{status} - {criterion}: {value}\")\n",
    "\n",
    "if all(c[1] for c in criteria):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUCCESS: Phase 3 Complete\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FAILED: Some criteria not met\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V278nVPzab3B"
   },
   "source": [
    "## Cell 15: Visualize Attention on Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0RRpTpHab3B"
   },
   "outputs": [],
   "source": [
    "# Visualize GNN attention on a subgraph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def visualize_gnn_attention(\n",
    "    subgraph: nx.DiGraph,\n",
    "    attention_scores: dict,\n",
    "    answer_entities: list,\n",
    "    question: str,\n",
    "    top_k: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize GNN attention on a subgraph.\n",
    "\n",
    "    Args:\n",
    "        subgraph: NetworkX DiGraph\n",
    "        attention_scores: Dict[node_name, float]\n",
    "        answer_entities: List of ground truth answer entities\n",
    "        question: Question text\n",
    "        top_k: Show only top-K nodes by attention\n",
    "    \"\"\"\n",
    "    # Get top-K nodes by attention\n",
    "    sorted_nodes = sorted(\n",
    "        attention_scores.items(), key=lambda x: x[1], reverse=True\n",
    "    )[:top_k]\n",
    "    top_nodes = [node for node, _ in sorted_nodes]\n",
    "\n",
    "    # Create subgraph with only top nodes\n",
    "    G_viz = subgraph.subgraph(top_nodes).copy()\n",
    "\n",
    "    # Node colors (red = answer, blue = high attention, gray = low attention)\n",
    "    node_colors = []\n",
    "    for node in G_viz.nodes():\n",
    "        if node in answer_entities:\n",
    "            node_colors.append(\"red\")\n",
    "        else:\n",
    "            # Scale by attention (darker = higher attention)\n",
    "            attn = attention_scores.get(node, 0.0)\n",
    "            intensity = min(attn * 10, 1.0)  # Scale for visibility\n",
    "            node_colors.append((0.2, 0.4, 0.8, 0.3 + 0.7 * intensity))\n",
    "\n",
    "    # Node sizes proportional to attention\n",
    "    node_sizes = [\n",
    "        300 + 2000 * attention_scores.get(node, 0.0) for node in G_viz.nodes()\n",
    "    ]\n",
    "\n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G_viz, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.title(f\"GNN Attention Visualization\\nQ: {question}\", fontsize=12)\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(\n",
    "        G_viz, pos, alpha=0.3, arrows=True, arrowsize=10, width=1.0\n",
    "    )\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(\n",
    "        G_viz, pos, node_color=node_colors, node_size=node_sizes, alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Draw labels (only for top 10)\n",
    "    labels = {node: node[:20] for node in list(G_viz.nodes())[:10]}\n",
    "    nx.draw_networkx_labels(G_viz, pos, labels, font_size=8)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print attention scores\n",
    "    print(\"Top 10 attention scores:\")\n",
    "    for i, (node, score) in enumerate(sorted_nodes[:10], 1):\n",
    "        is_answer = \"✓ ANSWER\" if node in answer_entities else \"\"\n",
    "        print(f\"  {i}. {node[:30]}: {score:.4f} {is_answer}\")\n",
    "\n",
    "\n",
    "# Test visualization\n",
    "test_question = \"Who is Barack Obama's spouse?\"\n",
    "test_answer = [\"Michelle Obama\", \"m.025s5v9\"]  # Freebase ID\n",
    "\n",
    "retrieved = retriever.retrieve(test_question)\n",
    "gnn_output = gnn_model.encode(retrieved, test_question)\n",
    "\n",
    "visualize_gnn_attention(\n",
    "    subgraph=retrieved.subgraph,\n",
    "    attention_scores=gnn_output.attention_scores,\n",
    "    answer_entities=test_answer,\n",
    "    question=test_question,\n",
    "    top_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uV1qeDzzab3C"
   },
   "source": [
    "## Cell 16: Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URJaPvgLab3C",
    "outputId": "b578e4b6-917f-4933-bafe-0357083787bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Summary:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"  Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    # Verify no memory leak\n",
    "    assert torch.cuda.memory_allocated() / 1e9 < 14.0, \"Memory leak detected!\"\n",
    "    print(\"\\n✓ Memory usage within acceptable range\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "041c254f9d784065ba72b6a1c896e2c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1979c36da93e4028bb631b21d6edd717",
      "max": 103,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c42782e59f3f42b6968ebd3b6cd26ba7",
      "value": 103
     }
    },
    "099dbc8612f94ba4933b3bdcc06d069b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1979c36da93e4028bb631b21d6edd717": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b6e2a07271c470682ed57cd9d4ec68c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_506ddfadd72c4909bb9a2114d4cbfffe",
       "IPY_MODEL_041c254f9d784065ba72b6a1c896e2c3",
       "IPY_MODEL_f3b2fcc49b6e499bb078f6cdd90a363f"
      ],
      "layout": "IPY_MODEL_f59f4bccda2c44ae9d15c4747b581a20"
     }
    },
    "506ddfadd72c4909bb9a2114d4cbfffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70b4723234ef4730b6e20a685ecf7a5a",
      "placeholder": "​",
      "style": "IPY_MODEL_099dbc8612f94ba4933b3bdcc06d069b",
      "value": "Loading weights: 100%"
     }
    },
    "51ea187fab3a4b57be40aef0109cd90f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70b4723234ef4730b6e20a685ecf7a5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c42782e59f3f42b6968ebd3b6cd26ba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c683e6e0e61a45dfb892cc701c70eb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3b2fcc49b6e499bb078f6cdd90a363f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51ea187fab3a4b57be40aef0109cd90f",
      "placeholder": "​",
      "style": "IPY_MODEL_c683e6e0e61a45dfb892cc701c70eb48",
      "value": " 103/103 [00:00&lt;00:00, 315.76it/s, Materializing param=pooler.dense.weight]"
     }
    },
    "f59f4bccda2c44ae9d15c4747b581a20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
