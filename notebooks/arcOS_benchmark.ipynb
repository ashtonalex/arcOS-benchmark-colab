{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQmG_saT2Xy9"
   },
   "source": [
    "# arcOS Benchmark - Causal QA with GNN + LLM\n",
    "\n",
    "**Phase 1: Environment & Data Foundation**\n",
    "\n",
    "This notebook implements the complete arcOS benchmark pipeline:\n",
    "- Graph Neural Network structural reasoning over knowledge graphs\n",
    "- LLM text generation with graph-guided prompts\n",
    "- Evaluation on RoG-WebQSP question answering dataset\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 or better)\n",
    "- Google Drive mounted for checkpointing\n",
    "- ~10GB free space on Drive\n",
    "\n",
    "**Architecture:**\n",
    "- Dataset: RoG-WebQSP (4,706 QA pairs with Freebase subgraphs)\n",
    "- Graph DB: NetworkX in-memory\n",
    "- GNN: Graph Attention Network (GATv2)\n",
    "- LLM: OpenRouter API (Claude 3.5 Sonnet)\n",
    "- Verbalization: Hard prompts (text-based, not soft embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Cloning Repository\n",
    "Update notebook with latest updates from GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing directory if it exists, then clone fresh\n",
    "!rm -rf /content/arcOS-benchmark-colab\n",
    "!git clone https://github.com/ashtonalex/arcOS-benchmark-colab /content/arcOS-benchmark-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD04nv-w2XzA"
   },
   "source": [
    "## Cell 1: Environment Setup\n",
    "\n",
    "Install dependencies and verify GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1Y0UZMb2XzA",
    "outputId": "e09f5891-3133-4beb-df1b-e7b584e58482"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP WITH UV PACKAGE MANAGER\n",
    "# Ensures absolute environment parity between kernel and installed packages\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab UV workaround: Clear broken constraint files\n",
    "os.environ[\"UV_CONSTRAINT\"] = \"\"\n",
    "os.environ[\"UV_BUILD_CONSTRAINT\"] = \"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: ENVIRONMENT PATH VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Capture current Python executable\n",
    "current_python = sys.executable\n",
    "print(f\"Current kernel executable: {current_python}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Site packages: {sys.path[0] if sys.path else 'N/A'}\")\n",
    "\n",
    "# Check if uv is available\n",
    "def check_uv_available():\n",
    "    \"\"\"Check if uv is installed and accessible.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['uv', '--version'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=5\n",
    "        )\n",
    "        return result.returncode == 0\n",
    "    except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "        return False\n",
    "\n",
    "uv_available = check_uv_available()\n",
    "\n",
    "if not uv_available:\n",
    "    print(\"\\n⚠ UV not found. Installing uv package manager...\")\n",
    "    %pip install -q uv\n",
    "    uv_available = check_uv_available()\n",
    "\n",
    "if uv_available:\n",
    "    # Get uv version\n",
    "    uv_version = subprocess.run(\n",
    "        ['uv', '--version'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    ).stdout.strip()\n",
    "    print(f\"✓ UV available: {uv_version}\")\n",
    "else:\n",
    "    print(\"✗ UV installation failed. Will fall back to pip.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PACKAGE INSTALLATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define packages to install\n",
    "packages = [\n",
    "    \"datasets\",\n",
    "    \"networkx\",\n",
    "    \"tqdm\",\n",
    "    \"faiss-gpu-cu12\" # Added faiss-gpu\n",
    "]\n",
    "\n",
    "# PyTorch with CUDA support\n",
    "torch_packages = \"torch torchvision torchaudio\"\n",
    "torch_index = \"https://download.pytorch.org/whl/cu118\"\n",
    "\n",
    "if uv_available:\n",
    "    print(f\"Installing packages using UV with --python {current_python}\\n\")\n",
    "\n",
    "    # Install PyTorch with CUDA\n",
    "    print(\"Installing PyTorch with CUDA 11.8 support...\")\n",
    "    !uv pip install --python {current_python} {torch_packages} --index-url {torch_index}\n",
    "\n",
    "    # Install other packages\n",
    "    print(\"\\nInstalling additional dependencies...\")\n",
    "    for package in packages:\n",
    "        !uv pip install --python {current_python} {package}\n",
    "else:\n",
    "    print(\"Falling back to standard pip installation\\n\")\n",
    "\n",
    "    # Install PyTorch with CUDA\n",
    "    print(\"Installing PyTorch with CUDA 11.8 support...\")\n",
    "    %pip install -q {torch_packages} --index-url {torch_index}\n",
    "\n",
    "    # Install other packages\n",
    "    print(\"\\nInstalling additional dependencies...\")\n",
    "    %pip install -q {' '.join(packages)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: INSTALLATION VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify installed packages are in the correct location\n",
    "def verify_package_location(package_name):\n",
    "    \"\"\"Verify package is installed in current kernel's site-packages.\"\"\"\n",
    "    try:\n",
    "        module = __import__(package_name)\n",
    "        module_path = Path(module.__file__).parent\n",
    "\n",
    "        # Check if module is in one of sys.path locations\n",
    "        in_sys_path = any(str(module_path).startswith(p) for p in sys.path if p)\n",
    "\n",
    "        # Get version if available\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "\n",
    "        return {\n",
    "            'installed': True,\n",
    "            'version': version,\n",
    "            'location': str(module_path),\n",
    "            'in_sys_path': in_sys_path\n",
    "        }\n",
    "    except ImportError:\n",
    "        return {'installed': False}\n",
    "\n",
    "# Verify key packages\n",
    "verification_packages = ['torch', 'datasets', 'networkx', 'tqdm', 'faiss'] # Added faiss for verification\n",
    "print(\"\\nVerifying installed packages:\\n\")\n",
    "\n",
    "all_verified = True\n",
    "for pkg in verification_packages:\n",
    "    info = verify_package_location(pkg)\n",
    "    if info['installed']:\n",
    "        status = \"✓\" if info['in_sys_path'] else \"⚠\"\n",
    "        print(f\"{status} {pkg:12s} v{info['version']:12s}\")\n",
    "        print(f\"  Location: {info['location']}\")\n",
    "        if not info['in_sys_path']:\n",
    "            print(f\"  WARNING: Not in sys.path!\")\n",
    "            all_verified = False\n",
    "    else:\n",
    "        print(f\"✗ {pkg:12s} NOT INSTALLED\")\n",
    "        all_verified = False\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: GPU VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"\\nGPU available: {gpu_available} {'✓' if gpu_available else '✗'}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠ Warning: No GPU detected.\")\n",
    "    print(\"  Go to: Runtime -> Change runtime type -> Select T4 GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENVIRONMENT SETUP SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Package manager: {'UV' if uv_available else 'pip'}\")\n",
    "print(f\"Python executable: {current_python}\")\n",
    "print(f\"All packages verified: {'✓ YES' if all_verified else '✗ NO'}\")\n",
    "print(f\"GPU available: {'✓ YES' if gpu_available else '✗ NO'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not all_verified:\n",
    "    print(\"\\n⚠ WARNING: Some packages failed verification. Check output above.\")\n",
    "else:\n",
    "    print(\"\\n✓ Environment setup complete with full parity!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv2OR5W52XzC"
   },
   "source": [
    "## Cell 2: Clean Room Import\n",
    "\n",
    "Purge bytecode caches, scrub `sys.modules`, pin `sys.path`, and verify source file integrity before importing any `src/` modules. Run this cell after every `git pull` or code change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqjQCB9b2XzC",
    "outputId": "4fcce3cf-0afe-4693-8394-755c927c26ed"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEAN ROOM IMPORT — guarantees fresh module loading\n",
    "# Run after every git pull / code edit to eliminate stale bytecode & caches\n",
    "# ============================================================================\n",
    "\n",
    "import sys, os, shutil, hashlib, importlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ── CONFIG ──────────────────────────────────────────────────────────────────\n",
    "REPO_ROOT = Path(\"/content/arcOS-benchmark-colab\")\n",
    "SRC_ROOT  = REPO_ROOT / \"src\"\n",
    "# Files to fingerprint (add any core logic files you want to verify)\n",
    "VERIFY_FILES = [\n",
    "    SRC_ROOT / \"config.py\",\n",
    "    SRC_ROOT / \"retrieval\" / \"pcst_solver.py\",\n",
    "    SRC_ROOT / \"gnn\" / \"encoder.py\",\n",
    "]\n",
    "# Module prefixes to scrub from sys.modules\n",
    "SCRUB_PREFIXES = (\"src\", \"src.\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLEAN ROOM IMPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ── STEP 1: Bytecode Purge ─────────────────────────────────────────────────\n",
    "print(\"\\n[1/4] Purging __pycache__ and .pyc files...\")\n",
    "cache_dirs_removed = 0\n",
    "pyc_files_removed  = 0\n",
    "\n",
    "for cache_dir in SRC_ROOT.rglob(\"__pycache__\"):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    cache_dirs_removed += 1\n",
    "\n",
    "for pyc_file in SRC_ROOT.rglob(\"*.pyc\"):\n",
    "    pyc_file.unlink()\n",
    "    pyc_files_removed += 1\n",
    "\n",
    "print(f\"  Removed {cache_dirs_removed} __pycache__ dirs, {pyc_files_removed} .pyc files\")\n",
    "\n",
    "# ── STEP 2: sys.modules Scrub ──────────────────────────────────────────────\n",
    "print(\"\\n[2/4] Scrubbing src.* from sys.modules...\")\n",
    "stale_keys = [k for k in sys.modules if k.startswith(SCRUB_PREFIXES)]\n",
    "for key in stale_keys:\n",
    "    del sys.modules[key]\n",
    "print(f\"  Evicted {len(stale_keys)} cached modules: {stale_keys[:8]}{'...' if len(stale_keys) > 8 else ''}\")\n",
    "\n",
    "# ── STEP 3: Path Priority ─────────────────────────────────────────────────\n",
    "print(\"\\n[3/4] Pinning sys.path priority...\")\n",
    "repo_str = str(REPO_ROOT)\n",
    "# Remove any existing entries to avoid duplicates\n",
    "sys.path = [p for p in sys.path if p != repo_str]\n",
    "# Insert at position 0 so our src/ wins over any pip-installed version\n",
    "sys.path.insert(0, repo_str)\n",
    "print(f\"  sys.path[0] = {sys.path[0]}\")\n",
    "\n",
    "# ── STEP 4: Source File Validation ─────────────────────────────────────────\n",
    "print(\"\\n[4/4] Verifying source file integrity...\")\n",
    "\n",
    "def file_fingerprint(path: Path) -> dict:\n",
    "    \"\"\"Return last-modified timestamp and MD5 hash for a file.\"\"\"\n",
    "    data = path.read_bytes()\n",
    "    md5  = hashlib.md5(data).hexdigest()\n",
    "    mtime = datetime.fromtimestamp(path.stat().st_mtime, tz=timezone.utc)\n",
    "    return {\"md5\": md5, \"modified\": mtime.strftime(\"%Y-%m-%d %H:%M:%S UTC\"), \"size\": len(data)}\n",
    "\n",
    "for fpath in VERIFY_FILES:\n",
    "    if fpath.exists():\n",
    "        info = file_fingerprint(fpath)\n",
    "        rel  = fpath.relative_to(REPO_ROOT)\n",
    "        print(f\"  ✓ {rel}\")\n",
    "        print(f\"    Modified : {info['modified']}\")\n",
    "        print(f\"    MD5      : {info['md5']}\")\n",
    "        print(f\"    Size     : {info['size']:,} bytes\")\n",
    "    else:\n",
    "        print(f\"  ⚠ {fpath} — NOT FOUND (skipped)\")\n",
    "\n",
    "# ── STEP 5: Fresh Imports ──────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Importing modules (fresh)...\")\n",
    "\n",
    "from src.config import BenchmarkConfig\n",
    "from src.utils.seeds import set_seeds\n",
    "from src.utils.checkpoints import (\n",
    "    ensure_drive_mounted,\n",
    "    checkpoint_exists,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    create_checkpoint_dirs,\n",
    ")\n",
    "from src.data.dataset_loader import RoGWebQSPLoader\n",
    "from src.data.graph_builder import GraphBuilder\n",
    "\n",
    "print(\"✓ All imports successful (clean load)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLEAN ROOM COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbv4KpD12XzD"
   },
   "source": [
    "## Cell 3: Configuration\n",
    "\n",
    "Initialize benchmark configuration with hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbf7sF7A2XzE",
    "outputId": "efae91da-0ceb-431d-91bf-e432271a18b5"
   },
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = BenchmarkConfig(\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    drive_root=\"/content/drive/MyDrive/arcOS_benchmark\",\n",
    ")\n",
    "\n",
    "# Print configuration summary\n",
    "config.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrmwQcua2XzG"
   },
   "source": [
    "## Cell 4: Seed Initialization\n",
    "\n",
    "Set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuvkZMYQ2XzH",
    "outputId": "9d0a2e8e-e4a2-48d7-e59c-deae42bcaaa1"
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds(config.seed, config.deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgEi3ieQ2XzH"
   },
   "source": [
    "## Cell 5: Google Drive Setup\n",
    "\n",
    "Mount Google Drive and create checkpoint/results directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjnHwHNG2XzI",
    "outputId": "f562ecf5-3f47-4e21-f4b2-f15035b69400"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive_mounted = ensure_drive_mounted()\n",
    "\n",
    "if drive_mounted:\n",
    "    # Create checkpoint and results directories\n",
    "    create_checkpoint_dirs(config.checkpoint_dir, config.results_dir)\n",
    "else:\n",
    "    print(\"⚠ Warning: Drive not mounted. Checkpointing will not work.\")\n",
    "    print(\"  Continuing with local /content/ storage (temporary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_PN8Rjb2XzI"
   },
   "source": [
    "## Cell 6: Dataset Loading\n",
    "\n",
    "Load RoG-WebQSP dataset from HuggingFace with Drive caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "HKOysb1o2XzJ",
    "outputId": "c1726f96-dc6e-424a-a92a-5b3847d29ffa"
   },
   "outputs": [],
   "source": [
    "# Initialize dataset loader\n",
    "cache_dir = config.checkpoint_dir / \"huggingface_cache\"\n",
    "loader = RoGWebQSPLoader(cache_dir=cache_dir)\n",
    "\n",
    "# Check for cached dataset\n",
    "dataset_checkpoint_path = config.get_checkpoint_path(\"dataset.pkl\")\n",
    "\n",
    "dataset = None # Initialize dataset to None\n",
    "\n",
    "if checkpoint_exists(dataset_checkpoint_path):\n",
    "    print(\"Loading dataset from checkpoint...\")\n",
    "    try:\n",
    "        dataset = load_checkpoint(dataset_checkpoint_path, format=\"pickle\")\n",
    "        print(\"✓ Dataset loaded from checkpoint.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠ Warning: Failed to load dataset from checkpoint due to missing files: {e}\")\n",
    "        print(\"  Falling back to downloading dataset from HuggingFace...\")\n",
    "        # If loading fails, proceed to download\n",
    "        pass # dataset remains None, so the next block will execute\n",
    "\n",
    "if dataset is None: # If dataset was not loaded successfully or checkpoint didn't exist\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    dataset = loader.load(dataset_name=config.dataset_name)\n",
    "    save_checkpoint(dataset, dataset_checkpoint_path, format=\"pickle\")\n",
    "    print(\"✓ Dataset downloaded and saved to checkpoint.\")\n",
    "\n",
    "# Slice dataset to desired sizes\n",
    "dataset = loader.slice_dataset(\n",
    "    dataset,\n",
    "    train_size=900,\n",
    "    val_size=90,\n",
    "    test_size=None  # Keep all test examples\n",
    ")\n",
    "\n",
    "# Inspect dataset schema\n",
    "loader.inspect_schema(dataset, num_examples=1)\n",
    "\n",
    "# Compute statistics\n",
    "loader.compute_statistics(dataset)\n",
    "\n",
    "# Validate split counts (updated for sliced dataset)\n",
    "split_valid = loader.validate_split_counts(\n",
    "    dataset,\n",
    "    expected_train=900,\n",
    "    expected_val=90,\n",
    "    expected_test=1628,  # Keep original test size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tpm668x2XzJ"
   },
   "source": [
    "## Cell 7: Graph Construction\n",
    "\n",
    "Build NetworkX graphs from dataset triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "UBzceOT52XzJ",
    "outputId": "fe7f4170-7b15-4b64-b334-10df45175d96"
   },
   "outputs": [],
   "source": [
    "# Initialize graph builder\n",
    "graph_builder = GraphBuilder(directed=config.graph_directed)\n",
    "\n",
    "# Check for cached unified graph\n",
    "unified_graph_path = config.get_checkpoint_path(\"unified_graph.pkl\")\n",
    "\n",
    "if checkpoint_exists(unified_graph_path):\n",
    "    print(\"Loading unified graph from checkpoint...\")\n",
    "    unified_graph = load_checkpoint(unified_graph_path, format=\"pickle\")\n",
    "else:\n",
    "    print(\"Building unified graph from training split...\")\n",
    "    unified_graph = graph_builder.build_unified_graph(dataset[\"train\"])\n",
    "    save_checkpoint(unified_graph, unified_graph_path, format=\"pickle\")\n",
    "\n",
    "# Print graph statistics\n",
    "graph_builder.print_graph_info(unified_graph, name=\"Unified Training Graph\")\n",
    "\n",
    "# Validate graph size\n",
    "graph_valid = graph_builder.validate_graph_size(\n",
    "    unified_graph,\n",
    "    min_nodes=config.unified_graph_min_nodes,\n",
    "    min_edges=config.unified_graph_min_edges,\n",
    ")\n",
    "\n",
    "# Build sample per-example graph for demonstration\n",
    "print(\"\\nBuilding sample per-example graph...\")\n",
    "sample_example = dataset[\"train\"][0]\n",
    "sample_graph = graph_builder.build_from_triples(\n",
    "    sample_example[\"graph\"],\n",
    "    graph_id=sample_example[\"id\"]\n",
    ")\n",
    "graph_builder.print_graph_info(sample_graph, name=\"Sample Per-Example Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pHieT482XzJ"
   },
   "source": [
    "## Cell 8: Phase 1 Validation\n",
    "\n",
    "Automated validation of all Phase 1 success criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEm6SVWK2XzK",
    "outputId": "3a4f0cd1-aaec-4e94-8118-be8d0aa20264"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 1 Success Criteria Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect validation results\n",
    "validation_results = {\n",
    "    \"GPU Available\": torch.cuda.is_available(),\n",
    "    \"All Imports Successful\": True,  # If we got here, imports worked\n",
    "    \"Dataset Splits Valid\": split_valid,\n",
    "    \"Unified Graph Size Valid\": graph_valid,\n",
    "}\n",
    "\n",
    "# Test checkpoint round-trip\n",
    "test_checkpoint_path = config.get_checkpoint_path(\"test_roundtrip.pkl\")\n",
    "test_data = {\"test\": \"round-trip\", \"value\": 42}\n",
    "try:\n",
    "    save_checkpoint(test_data, test_checkpoint_path, format=\"pickle\")\n",
    "    loaded_data = load_checkpoint(test_checkpoint_path, format=\"pickle\")\n",
    "    checkpoint_roundtrip_ok = (loaded_data == test_data)\n",
    "    validation_results[\"Checkpoint Round-Trip\"] = checkpoint_roundtrip_ok\n",
    "except Exception as e:\n",
    "    print(f\"Checkpoint round-trip failed: {e}\")\n",
    "    validation_results[\"Checkpoint Round-Trip\"] = False\n",
    "\n",
    "# Print results\n",
    "print(\"\\nValidation Results:\")\n",
    "all_passed = True\n",
    "for criterion, passed in validation_results.items():\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"  {status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_passed:\n",
    "    print(\"✓ PHASE 1 COMPLETE - All criteria passed!\")\n",
    "    print(\"\\nReady to proceed to Phase 2: Retrieval Pipeline\")\n",
    "else:\n",
    "    print(\"✗ PHASE 1 INCOMPLETE - Some criteria failed\")\n",
    "    print(\"\\nPlease review failed criteria above\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nPhase 1 Summary:\")\n",
    "print(f\"  Dataset: {config.dataset_name}\")\n",
    "print(f\"  Training examples: {len(dataset['train'])}\")\n",
    "print(f\"  Validation examples: {len(dataset['validation'])}\")\n",
    "print(f\"  Test examples: {len(dataset['test'])}\")\n",
    "print(f\"  Unified graph nodes: {unified_graph.number_of_nodes()}\")\n",
    "print(f\"  Unified graph edges: {unified_graph.number_of_edges()}\")\n",
    "print(f\"  Checkpoints saved to: {config.checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN_Yhru52XzK"
   },
   "source": [
    "## Cell 9: Build Retrieval Pipeline\n",
    "\n",
    "Initialize retrieval components (embeddings, FAISS index, PCST solver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install pcst-fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954,
     "referenced_widgets": [
      "3b6e2a07271c470682ed57cd9d4ec68c",
      "506ddfadd72c4909bb9a2114d4cbfffe",
      "041c254f9d784065ba72b6a1c896e2c3",
      "f3b2fcc49b6e499bb078f6cdd90a363f",
      "f59f4bccda2c44ae9d15c4747b581a20",
      "70b4723234ef4730b6e20a685ecf7a5a",
      "099dbc8612f94ba4933b3bdcc06d069b",
      "1979c36da93e4028bb631b21d6edd717",
      "c42782e59f3f42b6968ebd3b6cd26ba7",
      "51ea187fab3a4b57be40aef0109cd90f",
      "c683e6e0e61a45dfb892cc701c70eb48"
     ]
    },
    "id": "I48MWhc62XzL",
    "outputId": "a3da42c9-bd23-43d8-ac19-953603bb6553"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: RETRIEVAL PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from src.retrieval import Retriever\n",
    "\n",
    "# Build retriever (uses checkpoints if available)\n",
    "retriever = Retriever.build_from_checkpoint_or_new(\n",
    "    config=config,\n",
    "    unified_graph=unified_graph  # From Phase 1 Cell 7\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Retrieval pipeline initialized\")\n",
    "print(f\"  - Entity embeddings: {len(retriever.entity_index)} entities\")\n",
    "print(f\"  - Top-K: {config.top_k_entities}\")\n",
    "print(f\"  - PCST budget: {config.pcst_budget} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VwowPg62XzL"
   },
   "source": [
    "## Cell 10: Retrieval Validation\n",
    "\n",
    "Test retrieval pipeline on 10 validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "HfRB6T_p2XzL",
    "outputId": "e92539b3-a96d-4956-f2f9-dc61faaa5fd0"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RETRIEVAL VALIDATION (10 examples)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use first 10 validation examples\n",
    "val_examples = list(dataset[\"validation\"].select(range(10)))\n",
    "\n",
    "hit_count = 0\n",
    "total_time_ms = 0\n",
    "subgraph_sizes = []\n",
    "\n",
    "for i, example in enumerate(val_examples):\n",
    "    question = example[\"question\"]\n",
    "    answer_entities = example.get(\"a_entity\", [])\n",
    "    if isinstance(answer_entities, str):\n",
    "        answer_entities = [answer_entities]\n",
    "\n",
    "    # Extract topic entities from dataset\n",
    "    q_entities = example.get(\"q_entity\", [])\n",
    "    if isinstance(q_entities, str):\n",
    "        q_entities = [q_entities]\n",
    "\n",
    "    # Retrieve subgraph (q_entity used as primary seed when available)\n",
    "    result = retriever.retrieve(question, q_entity=q_entities)\n",
    "\n",
    "    # Check if answer entity in subgraph\n",
    "    subgraph_nodes = set(result.subgraph.nodes())\n",
    "    hit = any(ans in subgraph_nodes for ans in answer_entities)\n",
    "\n",
    "    if hit:\n",
    "        hit_count += 1\n",
    "\n",
    "    total_time_ms += result.retrieval_time_ms\n",
    "    subgraph_sizes.append(result.num_nodes)\n",
    "\n",
    "    # Print example\n",
    "    print(f\"\\n[{i+1}/10] Q: {question[:60]}...\")\n",
    "    print(f\"  Topic entities (q_entity): {q_entities}\")\n",
    "    print(f\"  Answer entities: {answer_entities}\")\n",
    "    print(f\"  Seeds used: {result.seed_entities[:5]}{'...' if len(result.seed_entities) > 5 else ''}\")\n",
    "    print(f\"  Subgraph: {result.num_nodes} nodes, {result.num_edges} edges\")\n",
    "    print(f\"  Hit: {'✓' if hit else '✗'}\")\n",
    "    print(f\"  Time: {result.retrieval_time_ms:.1f}ms\")\n",
    "\n",
    "# Summary metrics\n",
    "hit_rate = hit_count / len(val_examples) * 100\n",
    "avg_time = total_time_ms / len(val_examples)\n",
    "avg_size = sum(subgraph_sizes) / len(subgraph_sizes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Hit rate: {hit_rate:.1f}% ({hit_count}/{len(val_examples)})\")\n",
    "print(f\"Avg retrieval time: {avg_time:.1f}ms\")\n",
    "print(f\"Avg subgraph size: {avg_size:.1f} nodes\")\n",
    "print(f\"Max subgraph size: {max(subgraph_sizes)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju6e-sE22XzM"
   },
   "source": [
    "## Cell 11: Phase 2 Success Criteria\n",
    "\n",
    "Validate Phase 2 completion criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEDJ25h_2XzM",
    "outputId": "57355918-2aa3-4a8e-a4ef-5a0bfd4f4020"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2 SUCCESS CRITERIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criterion 1: Retrieval speed < 1 second\n",
    "speed_pass = avg_time < 1000  # ms\n",
    "print(f\"[{'✓' if speed_pass else '✗'}] Retrieval completes in <1 second: {avg_time:.1f}ms\")\n",
    "\n",
    "# Criterion 2: Hit rate > 60%\n",
    "hit_pass = hit_rate >= 60.0\n",
    "print(f\"[{'✓' if hit_pass else '✗'}] Subgraph contains answer entity >60%: {hit_rate:.1f}%\")\n",
    "\n",
    "# Criterion 3: All subgraphs connected\n",
    "all_connected = all(\n",
    "    nx.is_weakly_connected(\n",
    "        retriever.retrieve(\n",
    "            example[\"question\"],\n",
    "            q_entity=example.get(\"q_entity\")\n",
    "        ).subgraph\n",
    "    )\n",
    "    for example in val_examples[:5]  # Check first 5\n",
    ")\n",
    "print(f\"[{'✓' if all_connected else '✗'}] All subgraphs are connected\")\n",
    "\n",
    "# Criterion 4: Subgraph size respects budget\n",
    "size_pass = max(subgraph_sizes) <= config.pcst_budget\n",
    "print(f\"[{'✓' if size_pass else '✗'}] Subgraph size ≤ budget ({max(subgraph_sizes)} ≤ {config.pcst_budget})\")\n",
    "\n",
    "# Overall pass\n",
    "all_pass = speed_pass and hit_pass and all_connected and size_pass\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_pass:\n",
    "    print(\"✓ PHASE 2 COMPLETE - All criteria met!\")\n",
    "    print(\"\\nReady to proceed to Phase 3: GNN Encoder\")\n",
    "else:\n",
    "    print(\"⚠ PHASE 2 INCOMPLETE - Review failed criteria above\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdPaAEYiab3A"
   },
   "source": [
    "# Phase 3: GNN Encoder\n",
    "## Cell 12: Build/Load GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8YgdaqamjdWl",
    "outputId": "15f3fbd3-01fd-45be-dff2-21fef43fcace"
   },
   "outputs": [],
   "source": [
    "!uv pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbTg--JZkzUx",
    "outputId": "42a8b6fa-888d-4dc5-bc5a-664231ac08d8"
   },
   "outputs": [],
   "source": [
    "# Install torch_geometric and its dependencies\n",
    "print(\"Installing torch_geometric...\")\n",
    "!uv pip install torch_geometric torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.8.0+cu128.html\n",
    "print(\"✓ torch_geometric installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "kiad8bLiab3B",
    "outputId": "c1a86141-eb79-49d1-8de3-c4354834b1e3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PHASE 3: GNN Encoder\n",
    "# ============================================================\n",
    "\n",
    "print(\"Building GNN Model...\")\n",
    "print(\"This will either:\")\n",
    "print(\"  1. Load pre-trained model from checkpoint, OR\")\n",
    "print(\"  2. Prepare data and train from scratch (~30 min)\")\n",
    "print()\n",
    "\n",
    "from src.gnn import GNNModel\n",
    "\n",
    "# Build GNN model (handles checkpoint loading or training automatically)\n",
    "gnn_model = GNNModel.build_from_checkpoint_or_train(\n",
    "    config=config,\n",
    "    retriever=retriever,\n",
    "    train_data=dataset[\"train\"],\n",
    "    val_data=dataset[\"validation\"],\n",
    "    encoder_type=\"gatv2\",  # or \"graphsage\"\n",
    "    pooling_type=\"attention\",  # or \"mean\", \"max\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GNN Model Ready\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BbYDV0Zab3B"
   },
   "source": [
    "## Cell 13: Test GNN Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "J90pfuVTab3B",
    "outputId": "1d2ed4d0-9f08-4c45-a96c-07b57a64d634"
   },
   "outputs": [],
   "source": [
    "# Test GNN encoding on a single example\n",
    "print(\"Testing GNN inference on example query...\\n\")\n",
    "\n",
    "test_question = \"Who is Justin Bieber's brother?\"\n",
    "print(f\"Question: {test_question}\")\n",
    "\n",
    "# Retrieve subgraph\n",
    "retrieved = retriever.retrieve(test_question)\n",
    "print(f\"Retrieved subgraph: {retrieved.num_nodes} nodes, {retrieved.num_edges} edges\")\n",
    "\n",
    "# Encode with GNN\n",
    "gnn_output = gnn_model.encode(retrieved, test_question)\n",
    "\n",
    "print(f\"\\nGNN Output:\")\n",
    "print(f\"  Node embeddings shape: {gnn_output.node_embeddings.shape}\")\n",
    "print(f\"  Graph embedding shape: {gnn_output.graph_embedding.shape}\")\n",
    "print(f\"  Attention scores: {len(gnn_output.attention_scores)} nodes\")\n",
    "\n",
    "# Get top attention nodes\n",
    "top_nodes = gnn_model.get_top_attention_nodes(gnn_output, top_k=10)\n",
    "print(f\"\\nTop 10 nodes by attention score:\")\n",
    "for i, (node, score) in enumerate(top_nodes, 1):\n",
    "    print(f\"  {i}. {node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYEaOgU3ab3B"
   },
   "source": [
    "## Cell 14: Validate GNN Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lXQ_Uzgab3B"
   },
   "outputs": [],
   "source": [
    "# Load training history and display metrics\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_path = config.get_checkpoint_path(\"gnn_training_history.json\")\n",
    "with open(history_path, \"r\") as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 3 VALIDATION: GNN Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best metrics\n",
    "best_val_f1 = max(history[\"val_f1\"])\n",
    "best_val_loss = min(history[\"val_loss\"])\n",
    "final_val_f1 = history[\"val_f1\"][-1]\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Epochs trained: {len(history['train_loss'])}\")\n",
    "print(f\"  Best validation F1: {best_val_f1:.3f}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Final validation F1: {final_val_f1:.3f}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training and Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# F1 curve\n",
    "axes[1].plot(history[\"train_f1\"], label=\"Train F1\")\n",
    "axes[1].plot(history[\"val_f1\"], label=\"Val F1\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"F1 Score\")\n",
    "axes[1].set_title(\"Answer Node Prediction F1\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Success criteria check\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Success Criteria:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "criteria = [\n",
    "    (\"Validation F1 > 0.5\", best_val_f1 > 0.5, best_val_f1),\n",
    "    (\"Training completed < 30 min\", True, \"N/A\"),  # User observation\n",
    "    (\"No OOM errors\", True, \"N/A\"),  # User observation\n",
    "]\n",
    "\n",
    "for criterion, passed, value in criteria:\n",
    "    status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "    print(f\"{status} - {criterion}: {value}\")\n",
    "\n",
    "if all(c[1] for c in criteria):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUCCESS: Phase 3 Complete\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FAILED: Some criteria not met\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V278nVPzab3B"
   },
   "source": [
    "## Cell 15: Visualize Attention on Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0RRpTpHab3B"
   },
   "outputs": [],
   "source": [
    "# Visualize GNN attention on a subgraph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def visualize_gnn_attention(\n",
    "    subgraph: nx.DiGraph,\n",
    "    attention_scores: dict,\n",
    "    answer_entities: list,\n",
    "    question: str,\n",
    "    top_k: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize GNN attention on a subgraph.\n",
    "\n",
    "    Args:\n",
    "        subgraph: NetworkX DiGraph\n",
    "        attention_scores: Dict[node_name, float]\n",
    "        answer_entities: List of ground truth answer entities\n",
    "        question: Question text\n",
    "        top_k: Show only top-K nodes by attention\n",
    "    \"\"\"\n",
    "    # Get top-K nodes by attention\n",
    "    sorted_nodes = sorted(\n",
    "        attention_scores.items(), key=lambda x: x[1], reverse=True\n",
    "    )[:top_k]\n",
    "    top_nodes = [node for node, _ in sorted_nodes]\n",
    "\n",
    "    # Create subgraph with only top nodes\n",
    "    G_viz = subgraph.subgraph(top_nodes).copy()\n",
    "\n",
    "    # Node colors (red = answer, blue = high attention, gray = low attention)\n",
    "    node_colors = []\n",
    "    for node in G_viz.nodes():\n",
    "        if node in answer_entities:\n",
    "            node_colors.append(\"red\")\n",
    "        else:\n",
    "            # Scale by attention (darker = higher attention)\n",
    "            attn = attention_scores.get(node, 0.0)\n",
    "            intensity = min(attn * 10, 1.0)  # Scale for visibility\n",
    "            node_colors.append((0.2, 0.4, 0.8, 0.3 + 0.7 * intensity))\n",
    "\n",
    "    # Node sizes proportional to attention\n",
    "    node_sizes = [\n",
    "        300 + 2000 * attention_scores.get(node, 0.0) for node in G_viz.nodes()\n",
    "    ]\n",
    "\n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G_viz, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.title(f\"GNN Attention Visualization\\nQ: {question}\", fontsize=12)\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(\n",
    "        G_viz, pos, alpha=0.3, arrows=True, arrowsize=10, width=1.0\n",
    "    )\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(\n",
    "        G_viz, pos, node_color=node_colors, node_size=node_sizes, alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Draw labels (only for top 10)\n",
    "    labels = {node: node[:20] for node in list(G_viz.nodes())[:10]}\n",
    "    nx.draw_networkx_labels(G_viz, pos, labels, font_size=8)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print attention scores\n",
    "    print(\"Top 10 attention scores:\")\n",
    "    for i, (node, score) in enumerate(sorted_nodes[:10], 1):\n",
    "        is_answer = \"✓ ANSWER\" if node in answer_entities else \"\"\n",
    "        print(f\"  {i}. {node[:30]}: {score:.4f} {is_answer}\")\n",
    "\n",
    "\n",
    "# Test visualization\n",
    "test_question = \"Who is Barack Obama's spouse?\"\n",
    "test_answer = [\"Michelle Obama\", \"m.025s5v9\"]  # Freebase ID\n",
    "\n",
    "retrieved = retriever.retrieve(test_question)\n",
    "gnn_output = gnn_model.encode(retrieved, test_question)\n",
    "\n",
    "visualize_gnn_attention(\n",
    "    subgraph=retrieved.subgraph,\n",
    "    attention_scores=gnn_output.attention_scores,\n",
    "    answer_entities=test_answer,\n",
    "    question=test_question,\n",
    "    top_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uV1qeDzzab3C"
   },
   "source": [
    "## Cell 16: Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URJaPvgLab3C",
    "outputId": "b578e4b6-917f-4933-bafe-0357083787bb"
   },
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Summary:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"  Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    # Verify no memory leak\n",
    "    assert torch.cuda.memory_allocated() / 1e9 < 14.0, \"Memory leak detected!\"\n",
    "    print(\"\\n✓ Memory usage within acceptable range\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "041c254f9d784065ba72b6a1c896e2c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1979c36da93e4028bb631b21d6edd717",
      "max": 103,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c42782e59f3f42b6968ebd3b6cd26ba7",
      "value": 103
     }
    },
    "099dbc8612f94ba4933b3bdcc06d069b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1979c36da93e4028bb631b21d6edd717": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b6e2a07271c470682ed57cd9d4ec68c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_506ddfadd72c4909bb9a2114d4cbfffe",
       "IPY_MODEL_041c254f9d784065ba72b6a1c896e2c3",
       "IPY_MODEL_f3b2fcc49b6e499bb078f6cdd90a363f"
      ],
      "layout": "IPY_MODEL_f59f4bccda2c44ae9d15c4747b581a20"
     }
    },
    "506ddfadd72c4909bb9a2114d4cbfffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70b4723234ef4730b6e20a685ecf7a5a",
      "placeholder": "​",
      "style": "IPY_MODEL_099dbc8612f94ba4933b3bdcc06d069b",
      "value": "Loading weights: 100%"
     }
    },
    "51ea187fab3a4b57be40aef0109cd90f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70b4723234ef4730b6e20a685ecf7a5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c42782e59f3f42b6968ebd3b6cd26ba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c683e6e0e61a45dfb892cc701c70eb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3b2fcc49b6e499bb078f6cdd90a363f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51ea187fab3a4b57be40aef0109cd90f",
      "placeholder": "​",
      "style": "IPY_MODEL_c683e6e0e61a45dfb892cc701c70eb48",
      "value": " 103/103 [00:00&lt;00:00, 315.76it/s, Materializing param=pooler.dense.weight]"
     }
    },
    "f59f4bccda2c44ae9d15c4747b581a20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
