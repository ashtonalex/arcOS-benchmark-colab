{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arcOS Benchmark — Video Scene Graph QA with GNN + LLM\n",
    "\n",
    "This notebook implements the arcOS benchmark pipeline for video question answering:\n",
    "- **Data:** AGQA 2.0 QA pairs + Action Genome scene graph annotations\n",
    "- **Graph:** PyG HeteroData scene graphs (object nodes, spatial + temporal edges)\n",
    "- **Retrieval:** Per-video FAISS k-NN + PCST subgraph extraction\n",
    "- **GNN:** HeteroGATv2 encoder with per-edge-type attention\n",
    "- **Verbalization:** Attention-weighted triple formatting for LLM prompts\n",
    "- **Evaluation:** EM, F1, retrieval hit rate, attention precision\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 or better)\n",
    "- Google Drive mounted for checkpointing\n",
    "- ~5GB free space on Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Clone repository\n",
    "import os\n",
    "%cd /content\n",
    "!rm -rf arcOS-benchmark-colab\n",
    "!git clone https://github.com/ashtonalex/arcOS-benchmark-colab\n",
    "%cd /content/arcOS-benchmark-colab\n",
    "print('\\n✓ Repository cloned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install dependencies via uv\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Colab UV workaround\n",
    "os.environ['UV_CONSTRAINT'] = ''\n",
    "os.environ['UV_BUILD_CONSTRAINT'] = ''\n",
    "\n",
    "PY = sys.executable\n",
    "\n",
    "# Install uv if needed\n",
    "try:\n",
    "    subprocess.run(['uv', '--version'], capture_output=True, check=True)\n",
    "except (FileNotFoundError, subprocess.CalledProcessError):\n",
    "    %pip install -q uv\n",
    "\n",
    "# PyTorch + CUDA\n",
    "!uv pip install --python {PY} torch torchvision torchaudio\n",
    "\n",
    "# Core deps\n",
    "!uv pip install --python {PY} faiss-gpu-cu12 sentence-transformers gdown pcst-fast tqdm numpy\n",
    "\n",
    "# PyG + sparse/scatter\n",
    "!uv pip install --python {PY} torch_geometric torch_scatter torch_sparse \\\n",
    "    -f https://data.pyg.org/whl/torch-2.8.0+cu128.html\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "print(f'\\nGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"NOT AVAILABLE\"}')\n",
    "print('✓ Dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Clean room import\n",
    "import sys, shutil, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path('/content/arcOS-benchmark-colab')\n",
    "SRC_ROOT = REPO_ROOT / 'src'\n",
    "\n",
    "# Purge bytecode\n",
    "for d in SRC_ROOT.rglob('__pycache__'):\n",
    "    shutil.rmtree(d)\n",
    "\n",
    "# Scrub cached modules\n",
    "for k in [k for k in sys.modules if k.startswith(('src', 'src.'))]:\n",
    "    del sys.modules[k]\n",
    "\n",
    "# Pin path\n",
    "repo_str = str(REPO_ROOT)\n",
    "sys.path = [p for p in sys.path if p != repo_str]\n",
    "sys.path.insert(0, repo_str)\n",
    "\n",
    "# Fresh imports — all video pipeline modules\n",
    "from src.config import BenchmarkConfig\n",
    "from src.utils.seeds import set_seeds\n",
    "from src.utils.checkpoints import (\n",
    "    ensure_drive_mounted, checkpoint_exists,\n",
    "    save_checkpoint, load_checkpoint, create_checkpoint_dirs,\n",
    ")\n",
    "from src.data.agqa_loader import AGQALoader\n",
    "from src.data.ag_converter import load_ag_annotations, convert_all\n",
    "from src.data.scene_graph_builder import SceneGraphBuilder\n",
    "from src.retrieval.embeddings import TextEmbedder\n",
    "from src.retrieval.video_retriever import VideoRetriever\n",
    "from src.gnn.hetero_encoder import HeteroGATv2Encoder\n",
    "from src.gnn.hetero_trainer import HeteroGNNTrainer\n",
    "from src.gnn.hetero_model_wrapper import HeteroGNNModel\n",
    "from src.verbalization.scene_verbalizer import SceneVerbalizer\n",
    "from src.evaluation.benchmark import BenchmarkEvaluator\n",
    "\n",
    "print('✓ All video pipeline modules imported (clean load)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configuration\n",
    "config = BenchmarkConfig(\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    drive_root='/content/drive/MyDrive/arcOS_benchmark',\n",
    "    # Video scene graph overrides\n",
    "    agqa_subset_size=5000,\n",
    "    ag_frame_sample_rate=3,\n",
    "    top_k_seeds=10,\n",
    "    pcst_budget=70,\n",
    "    pcst_temporal_cost_weight=0.5,\n",
    "    gnn_hidden_dim=256,\n",
    "    gnn_num_layers=3,\n",
    "    gnn_num_heads=4,\n",
    "    gnn_encoder_type='hetero_gatv2',\n",
    "    # Training\n",
    "    num_epochs=10,\n",
    "    patience=5,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=16,\n",
    "    # Paths\n",
    "    ag_annotations_dir='/content/action_genome',\n",
    "    agqa_data_dir='/content/agqa',\n",
    ")\n",
    "\n",
    "config.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Seeds + Drive mount + checkpoint dirs\n",
    "set_seeds(config.seed, config.deterministic)\n",
    "\n",
    "drive_mounted = ensure_drive_mounted()\n",
    "if drive_mounted:\n",
    "    create_checkpoint_dirs(config.checkpoint_dir, config.results_dir)\n",
    "else:\n",
    "    print('WARNING: Drive not mounted. Checkpoints will use local storage.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: AGQA + Action Genome Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Download Action Genome annotations\n",
    "import gdown\n",
    "from pathlib import Path\n",
    "\n",
    "AG_DIR = Path(config.ag_annotations_dir)\n",
    "AG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Action Genome annotation files (Google Drive IDs)\n",
    "AG_FILES = {\n",
    "    'object_bbox_and_relationship.pkl': '1MBTtSbchUXTdhUzFyk-XfMSD25Y1c_8R',\n",
    "    'person_bbox.pkl': '1Vbg-hMaIBhbP4VVaKSA9UlG7l3ZoOPl0',\n",
    "    'frame_list.txt': '1mADeXW_cJbUfrroJMY-EhFPcfDNwKwfB',\n",
    "    'object_classes.txt': '1gMsXfK8ZdqvNB2XVDSWuixI2F3-jmFqq',\n",
    "    'relationship_classes.txt': '1lyalRkbSn1zVB8LFR5H6YjJF8SHYqLjr',\n",
    "}\n",
    "\n",
    "print('Downloading Action Genome annotations...')\n",
    "for filename, file_id in AG_FILES.items():\n",
    "    dest = AG_DIR / filename\n",
    "    if dest.exists():\n",
    "        print(f'  {filename}: already exists')\n",
    "    else:\n",
    "        print(f'  {filename}: downloading...')\n",
    "        gdown.download(id=file_id, output=str(dest), quiet=True)\n",
    "        print(f'  {filename}: done ({dest.stat().st_size / 1e6:.1f} MB)')\n",
    "\n",
    "print('\\n✓ Action Genome annotations ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Download AGQA 2.0 balanced QA pairs\n",
    "AGQA_DIR = Path(config.agqa_data_dir)\n",
    "AGQA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Downloading AGQA 2.0 balanced splits...')\n",
    "agqa_paths = AGQALoader.download_agqa(str(AGQA_DIR))\n",
    "\n",
    "print('\\n✓ AGQA data ready')\n",
    "for split, path in agqa_paths.items():\n",
    "    print(f'  {split}: {path} ({path.stat().st_size / 1e6:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Parse AGQA, subsample, split by video_id\n",
    "loader = AGQALoader(config)\n",
    "\n",
    "splits_path = config.get_checkpoint_path('agqa_splits.pkl')\n",
    "\n",
    "if checkpoint_exists(splits_path):\n",
    "    print('Loading AGQA splits from checkpoint...')\n",
    "    agqa_splits = load_checkpoint(splits_path, format='pickle')\n",
    "    train_samples = agqa_splits['train']\n",
    "    val_samples = agqa_splits['val']\n",
    "    test_samples = agqa_splits['test']\n",
    "else:\n",
    "    # Load all splits and merge\n",
    "    all_samples = []\n",
    "    for split, path in agqa_paths.items():\n",
    "        print(f'Loading {split}...')\n",
    "        samples = loader.load_from_file(str(path))\n",
    "        print(f'  {split}: {len(samples)} QA pairs')\n",
    "        all_samples.extend(samples)\n",
    "\n",
    "    print(f'\\nTotal QA pairs: {len(all_samples)}')\n",
    "\n",
    "    # Subsample\n",
    "    all_samples = loader.subsample(all_samples)\n",
    "    print(f'After subsample: {len(all_samples)}')\n",
    "\n",
    "    # Split by video_id (no leakage)\n",
    "    train_samples, val_samples, test_samples = loader.split(all_samples)\n",
    "\n",
    "    # Save\n",
    "    agqa_splits = {'train': train_samples, 'val': val_samples, 'test': test_samples}\n",
    "    save_checkpoint(agqa_splits, splits_path, format='pickle')\n",
    "\n",
    "print(f'\\nSplit sizes:')\n",
    "print(f'  Train: {len(train_samples)} QA pairs')\n",
    "print(f'  Val:   {len(val_samples)} QA pairs')\n",
    "print(f'  Test:  {len(test_samples)} QA pairs')\n",
    "\n",
    "# Video coverage\n",
    "train_vids = loader.get_unique_video_ids(train_samples)\n",
    "val_vids = loader.get_unique_video_ids(val_samples)\n",
    "test_vids = loader.get_unique_video_ids(test_samples)\n",
    "all_vids = train_vids | val_vids | test_vids\n",
    "\n",
    "print(f'\\nVideo coverage:')\n",
    "print(f'  Train videos: {len(train_vids)}')\n",
    "print(f'  Val videos:   {len(val_vids)}')\n",
    "print(f'  Test videos:  {len(test_vids)}')\n",
    "print(f'  Total unique: {len(all_vids)}')\n",
    "print(f'  Overlap train/val: {len(train_vids & val_vids)} (should be 0)')\n",
    "print(f'  Overlap train/test: {len(train_vids & test_vids)} (should be 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Convert AG annotations for AGQA video IDs\n",
    "import pickle\n",
    "\n",
    "converted_path = config.get_checkpoint_path('ag_converted.pkl')\n",
    "\n",
    "if checkpoint_exists(converted_path):\n",
    "    print('Loading converted AG annotations from checkpoint...')\n",
    "    ag_annotations = load_checkpoint(converted_path, format='pickle')\n",
    "else:\n",
    "    print('Loading raw AG annotations...')\n",
    "    raw_ag = load_ag_annotations(str(AG_DIR / 'object_bbox_and_relationship.pkl'))\n",
    "    print(f'  Raw AG: {len(raw_ag)} frame entries')\n",
    "\n",
    "    # Load class map if available\n",
    "    class_map = None\n",
    "    class_file = AG_DIR / 'object_classes.txt'\n",
    "    if class_file.exists():\n",
    "        with open(class_file) as f:\n",
    "            classes = [line.strip() for line in f if line.strip()]\n",
    "        class_map = {i: c for i, c in enumerate(classes)}\n",
    "        print(f'  Loaded {len(class_map)} object classes')\n",
    "\n",
    "    print(f'\\nConverting annotations for {len(all_vids)} videos...')\n",
    "    ag_annotations = convert_all(\n",
    "        raw_ag, all_vids,\n",
    "        frame_sample_rate=config.ag_frame_sample_rate,\n",
    "        class_map=class_map,\n",
    "    )\n",
    "    print(f'  Converted: {len(ag_annotations)} videos with frames')\n",
    "\n",
    "    save_checkpoint(ag_annotations, converted_path, format='pickle')\n",
    "    del raw_ag  # Free memory\n",
    "\n",
    "print(f'\\n✓ AG annotations: {len(ag_annotations)} videos')\n",
    "\n",
    "# Quick stats\n",
    "total_frames = sum(len(v['frames']) for v in ag_annotations.values())\n",
    "total_objects = sum(\n",
    "    sum(len(f['objects']) for f in v['frames'])\n",
    "    for v in ag_annotations.values()\n",
    ")\n",
    "total_relations = sum(\n",
    "    sum(len(f['relations']) for f in v['frames'])\n",
    "    for v in ag_annotations.values()\n",
    ")\n",
    "print(f'  Total frames: {total_frames}')\n",
    "print(f'  Total objects: {total_objects}')\n",
    "print(f'  Total relations: {total_relations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Data inspection\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Sample 3 videos\n",
    "rng = random.Random(config.seed)\n",
    "sample_vids = rng.sample(sorted(ag_annotations.keys()), min(3, len(ag_annotations)))\n",
    "\n",
    "print('=' * 60)\n",
    "print('DATA INSPECTION — Sample Videos')\n",
    "print('=' * 60)\n",
    "\n",
    "for vid in sample_vids:\n",
    "    ann = ag_annotations[vid]\n",
    "    frames = ann['frames']\n",
    "    print(f'\\n--- Video: {vid} ---')\n",
    "    print(f'  Frames: {len(frames)}')\n",
    "    \n",
    "    all_classes = []\n",
    "    all_preds = []\n",
    "    for f in frames:\n",
    "        for obj in f['objects']:\n",
    "            all_classes.append(obj['class'])\n",
    "        for rel in f['relations']:\n",
    "            all_preds.append(rel['predicate'])\n",
    "    \n",
    "    print(f'  Objects: {len(all_classes)} (unique classes: {len(set(all_classes))})')\n",
    "    print(f'  Relations: {len(all_preds)} (unique predicates: {len(set(all_preds))})')\n",
    "    \n",
    "    cls_counts = Counter(all_classes).most_common(5)\n",
    "    pred_counts = Counter(all_preds).most_common(5)\n",
    "    print(f'  Top classes: {cls_counts}')\n",
    "    print(f'  Top predicates: {pred_counts}')\n",
    "\n",
    "# Global distributions\n",
    "print('\\n' + '=' * 60)\n",
    "print('GLOBAL DISTRIBUTIONS')\n",
    "print('=' * 60)\n",
    "\n",
    "global_classes = Counter()\n",
    "global_preds = Counter()\n",
    "for ann in ag_annotations.values():\n",
    "    for f in ann['frames']:\n",
    "        for obj in f['objects']:\n",
    "            global_classes[obj['class']] += 1\n",
    "        for rel in f['relations']:\n",
    "            global_preds[rel['predicate']] += 1\n",
    "\n",
    "print(f'\\nObject classes ({len(global_classes)} unique):')\n",
    "for cls, cnt in global_classes.most_common(10):\n",
    "    print(f'  {cnt:6d}  {cls}')\n",
    "\n",
    "print(f'\\nRelation predicates ({len(global_preds)} unique):')\n",
    "for pred, cnt in global_preds.most_common(10):\n",
    "    print(f'  {cnt:6d}  {pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Phase 1 validation\n",
    "print('=' * 60)\n",
    "print('PHASE 1 VALIDATION')\n",
    "print('=' * 60)\n",
    "\n",
    "checks = {\n",
    "    'AG annotations loaded': len(ag_annotations) > 0,\n",
    "    'Train split non-empty': len(train_samples) > 0,\n",
    "    'Val split non-empty': len(val_samples) > 0,\n",
    "    'Test split non-empty': len(test_samples) > 0,\n",
    "    'No video leakage (train/val)': len(train_vids & val_vids) == 0,\n",
    "    'No video leakage (train/test)': len(train_vids & test_vids) == 0,\n",
    "    'Samples have required fields': all(\n",
    "        'question' in s and 'answer' in s and 'video_id' in s\n",
    "        for s in train_samples[:10]\n",
    "    ),\n",
    "    'Annotations have frames': all(\n",
    "        len(ag_annotations[vid]['frames']) > 0\n",
    "        for vid in list(ag_annotations.keys())[:10]\n",
    "    ),\n",
    "}\n",
    "\n",
    "all_pass = True\n",
    "for check, passed in checks.items():\n",
    "    status = '✓' if passed else '✗'\n",
    "    print(f'  {status} {check}')\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "if all_pass:\n",
    "    print('✓ PHASE 1 COMPLETE')\n",
    "else:\n",
    "    print('✗ PHASE 1 INCOMPLETE — review failed checks')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Scene Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Build HeteroData per video\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "graphs_path = config.get_checkpoint_path('scene_graphs.pkl')\n",
    "\n",
    "if checkpoint_exists(graphs_path):\n",
    "    print('Loading scene graphs from checkpoint...')\n",
    "    scene_graphs = load_checkpoint(graphs_path, format='pickle')\n",
    "else:\n",
    "    print('Building scene graphs with text embeddings...')\n",
    "    embedder = TextEmbedder(config)\n",
    "    builder = SceneGraphBuilder(config, embedder=embedder)\n",
    "\n",
    "    scene_graphs = {}\n",
    "    failed = 0\n",
    "    for vid in tqdm(sorted(ag_annotations.keys()), desc='Building graphs'):\n",
    "        try:\n",
    "            data = builder.build(ag_annotations[vid])\n",
    "            scene_graphs[vid] = data\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            if failed <= 5:\n",
    "                print(f'  Warning: {vid} failed: {e}')\n",
    "\n",
    "    if failed > 0:\n",
    "        print(f'  {failed} videos failed to build')\n",
    "\n",
    "    save_checkpoint(scene_graphs, graphs_path, format='pickle')\n",
    "\n",
    "    # Free embedder GPU memory\n",
    "    try:\n",
    "        embedder.model.to('cpu')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f'\\n✓ Scene graphs: {len(scene_graphs)} videos')\n",
    "\n",
    "# Quick stats\n",
    "import numpy as np\n",
    "node_counts = [g[\"object\"].x.shape[0] for g in scene_graphs.values()]\n",
    "spatial_counts = [g[\"object\", \"spatial_rel\", \"object\"].edge_index.shape[1] for g in scene_graphs.values()]\n",
    "temporal_counts = [g[\"object\", \"temporal\", \"object\"].edge_index.shape[1] for g in scene_graphs.values()]\n",
    "\n",
    "print(f'  Nodes — mean: {np.mean(node_counts):.0f}, min: {min(node_counts)}, max: {max(node_counts)}')\n",
    "print(f'  Spatial edges — mean: {np.mean(spatial_counts):.0f}, min: {min(spatial_counts)}, max: {max(spatial_counts)}')\n",
    "print(f'  Temporal edges — mean: {np.mean(temporal_counts):.0f}, min: {min(temporal_counts)}, max: {max(temporal_counts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Scene graph inspection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('=' * 60)\n",
    "print('SCENE GRAPH INSPECTION')\n",
    "print('=' * 60)\n",
    "\n",
    "# Sample graph details\n",
    "for vid in sample_vids[:2]:\n",
    "    if vid not in scene_graphs:\n",
    "        continue\n",
    "    g = scene_graphs[vid]\n",
    "    n_nodes = g['object'].x.shape[0]\n",
    "    n_spatial = g['object', 'spatial_rel', 'object'].edge_index.shape[1]\n",
    "    n_temporal = g['object', 'temporal', 'object'].edge_index.shape[1]\n",
    "    print(f'\\n  Video {vid}:')\n",
    "    print(f'    Nodes: {n_nodes}')\n",
    "    print(f'    Spatial edges: {n_spatial}')\n",
    "    print(f'    Temporal edges: {n_temporal}')\n",
    "    print(f'    Feature dim: {g[\"object\"].x.shape[1]}')\n",
    "    print(f'    Frames: {g.num_frames}')\n",
    "    if hasattr(g, 'object_names'):\n",
    "        unique_names = set(g.object_names)\n",
    "        print(f'    Unique object names: {len(unique_names)}')\n",
    "        print(f'    Sample names: {list(unique_names)[:8]}')\n",
    "\n",
    "# Size histogram\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].hist(node_counts, bins=30, color='#42A5F5', edgecolor='white')\n",
    "axes[0].set_xlabel('Nodes'); axes[0].set_ylabel('Videos'); axes[0].set_title('Node Count Distribution')\n",
    "\n",
    "axes[1].hist(spatial_counts, bins=30, color='#66BB6A', edgecolor='white')\n",
    "axes[1].set_xlabel('Edges'); axes[1].set_ylabel('Videos'); axes[1].set_title('Spatial Edge Distribution')\n",
    "\n",
    "axes[2].hist(temporal_counts, bins=30, color='#FFA726', edgecolor='white')\n",
    "axes[2].set_xlabel('Edges'); axes[2].set_ylabel('Videos'); axes[2].set_title('Temporal Edge Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Phase 2 validation\n",
    "print('=' * 60)\n",
    "print('PHASE 2 VALIDATION')\n",
    "print('=' * 60)\n",
    "\n",
    "# Check all AGQA videos have scene graphs\n",
    "missing_vids = all_vids - set(scene_graphs.keys())\n",
    "\n",
    "checks = {\n",
    "    'All AGQA videos have graphs': len(missing_vids) == 0,\n",
    "    'Both edge types present': all(\n",
    "        ('object', 'spatial_rel', 'object') in g.edge_types\n",
    "        and ('object', 'temporal', 'object') in g.edge_types\n",
    "        for g in list(scene_graphs.values())[:20]\n",
    "    ),\n",
    "    'Correct feature dim (384)': all(\n",
    "        g['object'].x.shape[1] == config.embedding_dim\n",
    "        for g in list(scene_graphs.values())[:20]\n",
    "    ),\n",
    "    'object_names stored': all(\n",
    "        hasattr(g, 'object_names') and g.object_names is not None\n",
    "        for g in list(scene_graphs.values())[:20]\n",
    "    ),\n",
    "    'spatial_predicates stored': all(\n",
    "        hasattr(g, 'spatial_predicates') and g.spatial_predicates is not None\n",
    "        for g in list(scene_graphs.values())[:20]\n",
    "    ),\n",
    "}\n",
    "\n",
    "all_pass = True\n",
    "for check, passed in checks.items():\n",
    "    status = '✓' if passed else '✗'\n",
    "    print(f'  {status} {check}')\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "\n",
    "if missing_vids:\n",
    "    print(f'  Missing videos: {list(missing_vids)[:5]}...')\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "if all_pass:\n",
    "    print('✓ PHASE 2 COMPLETE')\n",
    "else:\n",
    "    print('✗ PHASE 2 INCOMPLETE')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Per-Video Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: PCST configuration override\n",
    "# Edit values below then run this cell.\n",
    "\n",
    "config.pcst_budget = 70\n",
    "config.pcst_cost = 0.015\n",
    "config.pcst_pruning = 'none'\n",
    "config.top_k_seeds = 10\n",
    "config.pcst_temporal_cost_weight = 0.5\n",
    "\n",
    "config.__post_init__()  # re-validate\n",
    "\n",
    "print('PCST config applied:')\n",
    "print(f'  budget={config.pcst_budget}, cost={config.pcst_cost}')\n",
    "print(f'  pruning={config.pcst_pruning!r}, top_k_seeds={config.top_k_seeds}')\n",
    "print(f'  temporal_cost_weight={config.pcst_temporal_cost_weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Initialize VideoRetriever\n",
    "print('Initializing VideoRetriever...')\n",
    "embedder = TextEmbedder(config)\n",
    "retriever = VideoRetriever(config, embedder=embedder)\n",
    "print(f'✓ VideoRetriever ready (top_k_seeds={config.top_k_seeds}, pcst_budget={config.pcst_budget})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Validate retrieval on 50 val examples\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "n_val_test = min(50, len(val_samples))\n",
    "val_subset = val_samples[:n_val_test]\n",
    "\n",
    "print(f'Validating retrieval on {n_val_test} val examples...\\n')\n",
    "\n",
    "retrieval_times = []\n",
    "subgraph_sizes = []\n",
    "hit_count = 0\n",
    "\n",
    "for i, sample in enumerate(val_subset):\n",
    "    vid = sample['video_id']\n",
    "    if vid not in scene_graphs:\n",
    "        continue\n",
    "\n",
    "    result = retriever.retrieve(sample['question'], scene_graphs[vid])\n",
    "\n",
    "    retrieval_times.append(result.retrieval_time_ms)\n",
    "    subgraph_sizes.append(result.num_nodes)\n",
    "\n",
    "    # Check hit: answer text matches any object_name in subgraph\n",
    "    answer_text = sample['answer'].strip().lower()\n",
    "    subgraph_names = getattr(result.subgraph, 'object_names', []) or []\n",
    "    hit = any(name.strip().lower() == answer_text for name in subgraph_names)\n",
    "    if hit:\n",
    "        hit_count += 1\n",
    "\n",
    "    if i < 5:\n",
    "        print(f'  [{i+1}] Q: {sample[\"question\"][:60]}...')\n",
    "        print(f'       Nodes: {result.num_nodes}, Edges: {result.num_edges}, '\n",
    "              f'Time: {result.retrieval_time_ms:.0f}ms, Hit: {\"✓\" if hit else \"✗\"}')\n",
    "\n",
    "n_tested = len(retrieval_times)\n",
    "hit_rate = hit_count / n_tested * 100 if n_tested else 0\n",
    "avg_time = np.mean(retrieval_times) if retrieval_times else 0\n",
    "avg_size = np.mean(subgraph_sizes) if subgraph_sizes else 0\n",
    "\n",
    "print(f'\\n--- Retrieval Summary ({n_tested} examples) ---')\n",
    "print(f'  Hit rate: {hit_rate:.1f}% ({hit_count}/{n_tested})')\n",
    "print(f'  Avg time: {avg_time:.1f}ms')\n",
    "print(f'  Avg subgraph size: {avg_size:.1f} nodes')\n",
    "print(f'  Max subgraph size: {max(subgraph_sizes) if subgraph_sizes else 0} nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Diagnostics deep dive — trace single example\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick first val example\n",
    "probe = val_samples[0]\n",
    "probe_vid = probe['video_id']\n",
    "\n",
    "if probe_vid in scene_graphs:\n",
    "    print(f'Probe: Q=\"{probe[\"question\"]}\"')\n",
    "    print(f'       A=\"{probe[\"answer\"]}\"  Video={probe_vid}')\n",
    "\n",
    "    result = retriever.retrieve(probe['question'], scene_graphs[probe_vid])\n",
    "\n",
    "    print(f'\\nRetrieval result:')\n",
    "    print(f'  Seed indices: {result.seed_indices}')\n",
    "    print(f'  Subgraph nodes: {result.num_nodes}')\n",
    "    print(f'  Subgraph edges: {result.num_edges}')\n",
    "    print(f'  PCST used: {result.pcst_used}')\n",
    "    print(f'  Time: {result.retrieval_time_ms:.1f}ms')\n",
    "\n",
    "    if hasattr(result.subgraph, 'object_names'):\n",
    "        print(f'\\n  Object names in subgraph:')\n",
    "        for name in set(result.subgraph.object_names):\n",
    "            print(f'    - {name}')\n",
    "\n",
    "    # Funnel plot\n",
    "    sg = scene_graphs[probe_vid]\n",
    "    full_nodes = sg['object'].x.shape[0]\n",
    "    seeds = len(result.seed_indices)\n",
    "    output_nodes = result.num_nodes\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    labels = ['Full graph', 'k-NN seeds', 'PCST output']\n",
    "    values = [full_nodes, seeds, output_nodes]\n",
    "    colors = ['#1565C0', '#FFA726', '#66BB6A']\n",
    "    bars = ax.barh(labels[::-1], values[::-1], color=colors[::-1])\n",
    "    for bar, val in zip(bars, values[::-1]):\n",
    "        ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "                str(val), va='center', fontweight='bold')\n",
    "    ax.set_xlabel('Nodes')\n",
    "    ax.set_title('Retrieval Funnel')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f'Video {probe_vid} not in scene graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Phase 3 validation\n",
    "print('=' * 60)\n",
    "print('PHASE 3 VALIDATION')\n",
    "print('=' * 60)\n",
    "\n",
    "checks = {\n",
    "    'Retrieval speed < 1s': avg_time < 1000,\n",
    "    'Subgraph size <= budget': (max(subgraph_sizes) <= config.pcst_budget) if subgraph_sizes else False,\n",
    "    'Non-empty subgraphs': all(s > 0 for s in subgraph_sizes),\n",
    "}\n",
    "\n",
    "all_pass = True\n",
    "for check, passed in checks.items():\n",
    "    status = '✓' if passed else '✗'\n",
    "    print(f'  {status} {check}')\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "if all_pass:\n",
    "    print('✓ PHASE 3 COMPLETE')\n",
    "else:\n",
    "    print('✗ PHASE 3 INCOMPLETE')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Heterogeneous GNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Prepare PyG training data\n",
    "# This retrieves subgraphs for each QA pair and labels answer nodes.\n",
    "# Handled automatically by HeteroGNNModel.build_from_checkpoint_or_train.\n",
    "# (shown here for visibility; actual prep happens in Cell 25)\n",
    "\n",
    "print('PyG training data will be prepared in the next cell.')\n",
    "print(f'  Train examples: {len(train_samples)}')\n",
    "print(f'  Val examples: {len(val_samples)}')\n",
    "print(f'  Scene graphs available: {len(scene_graphs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Build/train HeteroGATv2 via HeteroGNNModel\n",
    "print('Building HeteroGNN model...\\n')\n",
    "\n",
    "hetero_model = HeteroGNNModel.build_from_checkpoint_or_train(\n",
    "    config=config,\n",
    "    retriever=retriever,\n",
    "    train_samples=train_samples,\n",
    "    val_samples=val_samples,\n",
    "    scene_graphs=scene_graphs,\n",
    ")\n",
    "\n",
    "# Set embedder for inference\n",
    "hetero_model.set_embedder(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Test inference on single example\n",
    "import torch\n",
    "\n",
    "print('Testing HeteroGNN inference...\\n')\n",
    "\n",
    "test_sample = val_samples[0]\n",
    "test_vid = test_sample['video_id']\n",
    "\n",
    "if test_vid in scene_graphs:\n",
    "    # Retrieve subgraph\n",
    "    test_result = retriever.retrieve(test_sample['question'], scene_graphs[test_vid])\n",
    "    subgraph = test_result.subgraph\n",
    "\n",
    "    print(f'Q: {test_sample[\"question\"]}')\n",
    "    print(f'A: {test_sample[\"answer\"]}')\n",
    "    print(f'Subgraph: {test_result.num_nodes} nodes, {test_result.num_edges} edges')\n",
    "\n",
    "    # Encode\n",
    "    node_emb, attn_scores, graph_emb = hetero_model.encode(subgraph, test_sample['question'])\n",
    "\n",
    "    print(f'\\nGNN output shapes:')\n",
    "    print(f'  Node embeddings: {node_emb.shape}')\n",
    "    print(f'  Attention scores: {attn_scores.shape}')\n",
    "    print(f'  Graph embedding: {graph_emb.shape}')\n",
    "\n",
    "    # Top attention nodes\n",
    "    top_k = min(10, len(attn_scores))\n",
    "    top_vals, top_idx = torch.topk(attn_scores, top_k)\n",
    "    names = getattr(subgraph, 'object_names', None)\n",
    "    print(f'\\nTop {top_k} nodes by attention:')\n",
    "    for i, (idx, val) in enumerate(zip(top_idx.tolist(), top_vals.tolist()), 1):\n",
    "        name = names[idx] if names and idx < len(names) else f'node_{idx}'\n",
    "        print(f'  {i}. {name}: {val:.4f}')\n",
    "\n",
    "    # Verify attention in [0, 1]\n",
    "    assert attn_scores.min() >= 0, f'Negative attention: {attn_scores.min()}'\n",
    "    assert attn_scores.max() <= 1.01, f'Attention > 1: {attn_scores.max()}'\n",
    "    print('\\n✓ Attention scores in valid range [0, 1]')\n",
    "else:\n",
    "    print(f'Video {test_vid} not in scene graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27: Training curves\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_path = config.get_checkpoint_path('hetero_gnn_history.json')\n",
    "\n",
    "try:\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss Curves'); axes[0].legend(); axes[0].grid(True)\n",
    "\n",
    "    axes[1].plot(history['train_f1'], label='Train F1')\n",
    "    axes[1].plot(history['val_f1'], label='Val F1')\n",
    "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('F1')\n",
    "    axes[1].set_title('F1 Curves'); axes[1].legend(); axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    best_f1 = max(history['val_f1'])\n",
    "    print(f'Best validation F1: {best_f1:.3f}')\n",
    "    print(f'Epochs trained: {len(history[\"train_loss\"])}')\n",
    "except FileNotFoundError:\n",
    "    print('No training history found (model loaded from checkpoint)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: Attention visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if test_vid in scene_graphs:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Attention distribution\n",
    "    ax = axes[0]\n",
    "    ax.hist(attn_scores.numpy(), bins=30, color='#42A5F5', edgecolor='white')\n",
    "    ax.set_xlabel('Attention Score')\n",
    "    ax.set_ylabel('Nodes')\n",
    "    ax.set_title(f'Attention Distribution ({len(attn_scores)} nodes)')\n",
    "\n",
    "    # Attention by object class\n",
    "    ax = axes[1]\n",
    "    if names:\n",
    "        from collections import defaultdict\n",
    "        class_scores = defaultdict(list)\n",
    "        for idx, score in enumerate(attn_scores.tolist()):\n",
    "            if idx < len(names):\n",
    "                class_scores[names[idx]].append(score)\n",
    "        class_means = {k: np.mean(v) for k, v in class_scores.items()}\n",
    "        sorted_classes = sorted(class_means.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        labels, values = zip(*sorted_classes) if sorted_classes else ([], [])\n",
    "        ax.barh(list(labels)[::-1], list(values)[::-1], color='#66BB6A')\n",
    "        ax.set_xlabel('Mean Attention')\n",
    "        ax.set_title('Attention by Object Class (top 15)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29: Memory check\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU Memory:')\n",
    "    print(f'  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "    print(f'  Reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB')\n",
    "    print(f'  Max alloc: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB')\n",
    "    assert torch.cuda.memory_allocated() / 1e9 < 14.0, 'Memory leak!'\n",
    "    print('  ✓ Memory OK')\n",
    "else:\n",
    "    print('GPU not available — running on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30: Phase 4 validation\n",
    "print('=' * 60)\n",
    "print('PHASE 4 VALIDATION')\n",
    "print('=' * 60)\n",
    "\n",
    "# Check training history for F1\n",
    "try:\n",
    "    best_val_f1 = max(history['val_f1'])\n",
    "    f1_pass = best_val_f1 > 0.1  # Relaxed threshold for scene graphs\n",
    "except Exception:\n",
    "    best_val_f1 = None\n",
    "    f1_pass = True  # Skip if loaded from checkpoint\n",
    "\n",
    "checks = {\n",
    "    'Model loads/trains successfully': hetero_model is not None,\n",
    "    'Inference produces valid shapes': node_emb.shape[1] == config.gnn_hidden_dim,\n",
    "    'Attention in [0, 1]': attn_scores.min() >= 0 and attn_scores.max() <= 1.01,\n",
    "    'No OOM': True,  # If we got here, no OOM\n",
    "}\n",
    "if best_val_f1 is not None:\n",
    "    checks[f'Val F1 > threshold ({best_val_f1:.3f})'] = f1_pass\n",
    "\n",
    "all_pass = True\n",
    "for check, passed in checks.items():\n",
    "    status = '✓' if passed else '✗'\n",
    "    print(f'  {status} {check}')\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "if all_pass:\n",
    "    print('✓ PHASE 4 COMPLETE')\n",
    "else:\n",
    "    print('✗ PHASE 4 INCOMPLETE')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Verbalization & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 32: Verbalize examples — weighted vs unweighted side-by-side\n",
    "verbalizer = SceneVerbalizer(config)\n",
    "\n",
    "print('=' * 60)\n",
    "print('VERBALIZATION EXAMPLES')\n",
    "print('=' * 60)\n",
    "\n",
    "for i, sample in enumerate(val_samples[:3]):\n",
    "    vid = sample['video_id']\n",
    "    if vid not in scene_graphs:\n",
    "        continue\n",
    "\n",
    "    result = retriever.retrieve(sample['question'], scene_graphs[vid])\n",
    "    node_emb, attn_scores_i, graph_emb = hetero_model.encode(\n",
    "        result.subgraph, sample['question']\n",
    "    )\n",
    "\n",
    "    weighted = verbalizer.verbalize(result.subgraph, attn_scores_i)\n",
    "    unweighted = verbalizer.verbalize_unweighted(result.subgraph)\n",
    "\n",
    "    print(f'\\n--- Example {i+1} ---')\n",
    "    print(f'Q: {sample[\"question\"]}')\n",
    "    print(f'A: {sample[\"answer\"]}')\n",
    "    print(f'\\nAttention-weighted verbalization:')\n",
    "    print(weighted[:500] if weighted else '  (empty)')\n",
    "    print(f'\\nUnweighted verbalization:')\n",
    "    print(unweighted[:500] if unweighted else '  (empty)')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 33: Run evaluation on test subset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evaluator = BenchmarkEvaluator()\n",
    "\n",
    "n_test = min(100, len(test_samples))\n",
    "test_subset = test_samples[:n_test]\n",
    "\n",
    "print(f'Evaluating on {n_test} test examples...\\n')\n",
    "\n",
    "eval_results = []\n",
    "for sample in tqdm(test_subset, desc='Evaluating'):\n",
    "    vid = sample['video_id']\n",
    "    if vid not in scene_graphs:\n",
    "        continue\n",
    "\n",
    "    # Retrieve\n",
    "    result = retriever.retrieve(sample['question'], scene_graphs[vid])\n",
    "    subgraph = result.subgraph\n",
    "\n",
    "    # Encode\n",
    "    node_emb, attn_scores_i, graph_emb = hetero_model.encode(subgraph, sample['question'])\n",
    "\n",
    "    # Find answer nodes by name matching\n",
    "    answer_text = sample['answer'].strip().lower()\n",
    "    names = getattr(subgraph, 'object_names', []) or []\n",
    "    answer_nodes = [i for i, n in enumerate(names) if n.strip().lower() == answer_text]\n",
    "\n",
    "    # Compute retrieval metrics\n",
    "    selected = list(range(subgraph['object'].x.shape[0]))\n",
    "    hit_rate = evaluator.retrieval_hit_rate(selected, answer_nodes)\n",
    "    attn_prec = evaluator.attention_precision(attn_scores_i, answer_nodes, top_k=5)\n",
    "\n",
    "    eval_results.append({\n",
    "        'retrieval_hit_rate': hit_rate,\n",
    "        'attention_precision': attn_prec,\n",
    "    })\n",
    "\n",
    "# Aggregate\n",
    "if eval_results:\n",
    "    agg = evaluator.aggregate(eval_results)\n",
    "\n",
    "    print(f'\\n--- Evaluation Results ({len(eval_results)} examples) ---')\n",
    "    for metric, value in agg.items():\n",
    "        print(f'  {metric}: {value:.4f}')\n",
    "else:\n",
    "    print('No examples evaluated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34: Results visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if eval_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Bar chart of aggregate metrics\n",
    "    ax = axes[0]\n",
    "    metrics = list(agg.keys())\n",
    "    values = list(agg.values())\n",
    "    colors = ['#42A5F5', '#66BB6A', '#FFA726', '#EF5350'][:len(metrics)]\n",
    "    bars = ax.bar(metrics, values, color=colors)\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{val:.3f}', ha='center', fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Aggregate Metrics')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Distribution of per-example scores\n",
    "    ax = axes[1]\n",
    "    hit_rates = [r['retrieval_hit_rate'] for r in eval_results]\n",
    "    attn_precs = [r['attention_precision'] for r in eval_results]\n",
    "    ax.hist(hit_rates, bins=20, alpha=0.6, label='Hit Rate', color='#42A5F5')\n",
    "    ax.hist(attn_precs, bins=20, alpha=0.6, label='Attention Precision', color='#66BB6A')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Examples')\n",
    "    ax.set_title('Per-Example Score Distribution')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 35: Save results to Drive\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'agqa_subset_size': config.agqa_subset_size,\n",
    "        'ag_frame_sample_rate': config.ag_frame_sample_rate,\n",
    "        'pcst_budget': config.pcst_budget,\n",
    "        'top_k_seeds': config.top_k_seeds,\n",
    "        'gnn_hidden_dim': config.gnn_hidden_dim,\n",
    "        'gnn_num_layers': config.gnn_num_layers,\n",
    "        'gnn_num_heads': config.gnn_num_heads,\n",
    "        'gnn_encoder_type': config.gnn_encoder_type,\n",
    "    },\n",
    "    'data_stats': {\n",
    "        'train_samples': len(train_samples),\n",
    "        'val_samples': len(val_samples),\n",
    "        'test_samples': len(test_samples),\n",
    "        'scene_graphs': len(scene_graphs),\n",
    "    },\n",
    "    'metrics': agg if eval_results else {},\n",
    "    'n_evaluated': len(eval_results),\n",
    "}\n",
    "\n",
    "results_path = config.get_results_path('video_benchmark_results.json')\n",
    "save_checkpoint(results_data, results_path, format='json')\n",
    "print(f'Results saved to {results_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 36: Final summary\n",
    "print('=' * 60)\n",
    "print('arcOS BENCHMARK — FINAL SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "phases = {\n",
    "    'Phase 1: Data Loading': len(ag_annotations) > 0 and len(train_samples) > 0,\n",
    "    'Phase 2: Scene Graphs': len(scene_graphs) > 0,\n",
    "    'Phase 3: Retrieval': len(retrieval_times) > 0,\n",
    "    'Phase 4: HeteroGNN': hetero_model is not None,\n",
    "    'Phase 5: Evaluation': len(eval_results) > 0,\n",
    "}\n",
    "\n",
    "for phase, passed in phases.items():\n",
    "    status = '✓' if passed else '✗'\n",
    "    print(f'  {status} {phase}')\n",
    "\n",
    "if eval_results:\n",
    "    print(f'\\nKey Metrics:')\n",
    "    for metric, value in agg.items():\n",
    "        print(f'  {metric}: {value:.4f}')\n",
    "\n",
    "print(f'\\nData:')\n",
    "print(f'  Videos: {len(scene_graphs)}')\n",
    "print(f'  QA pairs: {len(train_samples) + len(val_samples) + len(test_samples)}')\n",
    "print(f'  Test evaluated: {len(eval_results)}')\n",
    "\n",
    "all_complete = all(phases.values())\n",
    "print('\\n' + '=' * 60)\n",
    "if all_complete:\n",
    "    print('✓ BENCHMARK COMPLETE')\n",
    "else:\n",
    "    print('✗ BENCHMARK INCOMPLETE — review phase statuses above')\n",
    "print('=' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
