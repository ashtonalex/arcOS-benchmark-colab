{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQmG_saT2Xy9"
   },
   "source": [
    "# arcOS Benchmark - Causal QA with GNN + LLM\n",
    "\n",
    "**Phase 1: Environment & Data Foundation**\n",
    "\n",
    "This notebook implements the complete arcOS benchmark pipeline:\n",
    "- Graph Neural Network structural reasoning over knowledge graphs\n",
    "- LLM text generation with graph-guided prompts\n",
    "- Evaluation on RoG-WebQSP question answering dataset\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 or better)\n",
    "- Google Drive mounted for checkpointing\n",
    "- ~10GB free space on Drive\n",
    "\n",
    "**Architecture:**\n",
    "- Dataset: RoG-WebQSP (4,706 QA pairs with Freebase subgraphs)\n",
    "- Graph DB: NetworkX in-memory\n",
    "- GNN: Graph Attention Network (GATv2)\n",
    "- LLM: OpenRouter API (Claude 3.5 Sonnet)\n",
    "- Verbalization: Hard prompts (text-based, not soft embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Cloning Repository\n",
    "Update notebook with latest updates from GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/arcOS-benchmark-colab'...\n",
      "remote: Enumerating objects: 201, done.\u001b[K\n",
      "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
      "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
      "remote: Total 201 (delta 100), reused 157 (delta 59), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (201/201), 216.26 KiB | 16.63 MiB/s, done.\n",
      "Resolving deltas: 100% (100/100), done.\n"
     ]
    }
   ],
   "source": [
    "# Remove existing directory if it exists, then clone fresh\n",
    "!rm -rf /content/arcOS-benchmark-colab\n",
    "!git clone https://github.com/ashtonalex/arcOS-benchmark-colab /content/arcOS-benchmark-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD04nv-w2XzA"
   },
   "source": [
    "## Cell 1: Environment Setup\n",
    "\n",
    "Install dependencies and verify GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1Y0UZMb2XzA",
    "outputId": "e09f5891-3133-4beb-df1b-e7b584e58482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: ENVIRONMENT PATH VERIFICATION\n",
      "======================================================================\n",
      "Current kernel executable: /usr/bin/python3\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "Site packages: /content\n",
      "✓ UV available: uv 0.9.26\n",
      "\n",
      "======================================================================\n",
      "STEP 2: PACKAGE INSTALLATION\n",
      "======================================================================\n",
      "Installing packages using UV with --python /usr/bin/python3\n",
      "\n",
      "Installing PyTorch with CUDA 11.8 support...\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 165ms\u001b[0m\u001b[0m\n",
      "\n",
      "Installing additional dependencies...\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 121ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 9.97s\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0mcu12==12.9.79                    \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfaiss-gpu-cu12\u001b[0m\u001b[2m==1.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.9.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.9.79\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "STEP 3: INSTALLATION VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "Verifying installed packages:\n",
      "\n",
      "✓ torch        v2.9.0+cpu   \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/torch\n",
      "\n",
      "✓ datasets     v4.0.0       \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/datasets\n",
      "\n",
      "✓ networkx     v3.6.1       \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/networkx\n",
      "\n",
      "✓ tqdm         v4.67.3      \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/tqdm\n",
      "\n",
      "✓ faiss        v1.13.2      \n",
      "  Location: /usr/local/lib/python3.12/dist-packages/faiss\n",
      "\n",
      "======================================================================\n",
      "STEP 4: GPU VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "GPU available: False ✗\n",
      "⚠ Warning: No GPU detected.\n",
      "  Go to: Runtime -> Change runtime type -> Select T4 GPU\n",
      "\n",
      "======================================================================\n",
      "ENVIRONMENT SETUP SUMMARY\n",
      "======================================================================\n",
      "Package manager: UV\n",
      "Python executable: /usr/bin/python3\n",
      "All packages verified: ✓ YES\n",
      "GPU available: ✗ NO\n",
      "======================================================================\n",
      "\n",
      "✓ Environment setup complete with full parity!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP WITH UV PACKAGE MANAGER\n",
    "# Ensures absolute environment parity between kernel and installed packages\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab UV workaround: Clear broken constraint files\n",
    "os.environ[\"UV_CONSTRAINT\"] = \"\"\n",
    "os.environ[\"UV_BUILD_CONSTRAINT\"] = \"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: ENVIRONMENT PATH VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Capture current Python executable\n",
    "current_python = sys.executable\n",
    "print(f\"Current kernel executable: {current_python}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Site packages: {sys.path[0] if sys.path else 'N/A'}\")\n",
    "\n",
    "# Check if uv is available\n",
    "def check_uv_available():\n",
    "    \"\"\"Check if uv is installed and accessible.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['uv', '--version'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=5\n",
    "        )\n",
    "        return result.returncode == 0\n",
    "    except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "        return False\n",
    "\n",
    "uv_available = check_uv_available()\n",
    "\n",
    "if not uv_available:\n",
    "    print(\"\\n⚠ UV not found. Installing uv package manager...\")\n",
    "    %pip install -q uv\n",
    "    uv_available = check_uv_available()\n",
    "\n",
    "if uv_available:\n",
    "    # Get uv version\n",
    "    uv_version = subprocess.run(\n",
    "        ['uv', '--version'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    ).stdout.strip()\n",
    "    print(f\"✓ UV available: {uv_version}\")\n",
    "else:\n",
    "    print(\"✗ UV installation failed. Will fall back to pip.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PACKAGE INSTALLATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define packages to install\n",
    "packages = [\n",
    "    \"datasets\",\n",
    "    \"networkx\",\n",
    "    \"tqdm\",\n",
    "    \"faiss-gpu-cu12\" # Added faiss-gpu\n",
    "]\n",
    "\n",
    "# PyTorch with CUDA support\n",
    "torch_packages = \"torch torchvision torchaudio\"\n",
    "torch_index = \"https://download.pytorch.org/whl/cu118\"\n",
    "\n",
    "if uv_available:\n",
    "    print(f\"Installing packages using UV with --python {current_python}\\n\")\n",
    "\n",
    "    # Install PyTorch with CUDA\n",
    "    print(\"Installing PyTorch with CUDA 11.8 support...\")\n",
    "    !uv pip install --python {current_python} {torch_packages} --index-url {torch_index}\n",
    "\n",
    "    # Install other packages\n",
    "    print(\"\\nInstalling additional dependencies...\")\n",
    "    for package in packages:\n",
    "        !uv pip install --python {current_python} {package}\n",
    "else:\n",
    "    print(\"Falling back to standard pip installation\\n\")\n",
    "\n",
    "    # Install PyTorch with CUDA\n",
    "    print(\"Installing PyTorch with CUDA 11.8 support...\")\n",
    "    %pip install -q {torch_packages} --index-url {torch_index}\n",
    "\n",
    "    # Install other packages\n",
    "    print(\"\\nInstalling additional dependencies...\")\n",
    "    %pip install -q {' '.join(packages)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: INSTALLATION VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify installed packages are in the correct location\n",
    "def verify_package_location(package_name):\n",
    "    \"\"\"Verify package is installed in current kernel's site-packages.\"\"\"\n",
    "    try:\n",
    "        module = __import__(package_name)\n",
    "        module_path = Path(module.__file__).parent\n",
    "\n",
    "        # Check if module is in one of sys.path locations\n",
    "        in_sys_path = any(str(module_path).startswith(p) for p in sys.path if p)\n",
    "\n",
    "        # Get version if available\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "\n",
    "        return {\n",
    "            'installed': True,\n",
    "            'version': version,\n",
    "            'location': str(module_path),\n",
    "            'in_sys_path': in_sys_path\n",
    "        }\n",
    "    except ImportError:\n",
    "        return {'installed': False}\n",
    "\n",
    "# Verify key packages\n",
    "verification_packages = ['torch', 'datasets', 'networkx', 'tqdm', 'faiss'] # Added faiss for verification\n",
    "print(\"\\nVerifying installed packages:\\n\")\n",
    "\n",
    "all_verified = True\n",
    "for pkg in verification_packages:\n",
    "    info = verify_package_location(pkg)\n",
    "    if info['installed']:\n",
    "        status = \"✓\" if info['in_sys_path'] else \"⚠\"\n",
    "        print(f\"{status} {pkg:12s} v{info['version']:12s}\")\n",
    "        print(f\"  Location: {info['location']}\")\n",
    "        if not info['in_sys_path']:\n",
    "            print(f\"  WARNING: Not in sys.path!\")\n",
    "            all_verified = False\n",
    "    else:\n",
    "        print(f\"✗ {pkg:12s} NOT INSTALLED\")\n",
    "        all_verified = False\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: GPU VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"\\nGPU available: {gpu_available} {'✓' if gpu_available else '✗'}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠ Warning: No GPU detected.\")\n",
    "    print(\"  Go to: Runtime -> Change runtime type -> Select T4 GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENVIRONMENT SETUP SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Package manager: {'UV' if uv_available else 'pip'}\")\n",
    "print(f\"Python executable: {current_python}\")\n",
    "print(f\"All packages verified: {'✓ YES' if all_verified else '✗ NO'}\")\n",
    "print(f\"GPU available: {'✓ YES' if gpu_available else '✗ NO'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not all_verified:\n",
    "    print(\"\\n⚠ WARNING: Some packages failed verification. Check output above.\")\n",
    "else:\n",
    "    print(\"\\n✓ Environment setup complete with full parity!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv2OR5W52XzC"
   },
   "source": "## Cell 2: Clean Room Import\n\nPurge bytecode caches, scrub `sys.modules`, pin `sys.path`, and verify source file integrity before importing any `src/` modules. Run this cell after every `git pull` or code change."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqjQCB9b2XzC",
    "outputId": "4fcce3cf-0afe-4693-8394-755c927c26ed"
   },
   "outputs": [],
   "source": "# ============================================================================\n# CLEAN ROOM IMPORT — guarantees fresh module loading\n# Run after every git pull / code edit to eliminate stale bytecode & caches\n# ============================================================================\n\nimport sys, os, shutil, hashlib, importlib\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n# ── CONFIG ──────────────────────────────────────────────────────────────────\nREPO_ROOT = Path(\"/content/arcOS-benchmark-colab\")\nSRC_ROOT  = REPO_ROOT / \"src\"\n# Files to fingerprint (add any core logic files you want to verify)\nVERIFY_FILES = [\n    SRC_ROOT / \"config.py\",\n    SRC_ROOT / \"retrieval\" / \"pcst_solver.py\",\n    SRC_ROOT / \"gnn\" / \"encoder.py\",\n]\n# Module prefixes to scrub from sys.modules\nSCRUB_PREFIXES = (\"src\", \"src.\")\n\nprint(\"=\" * 70)\nprint(\"CLEAN ROOM IMPORT\")\nprint(\"=\" * 70)\n\n# ── STEP 1: Bytecode Purge ─────────────────────────────────────────────────\nprint(\"\\n[1/4] Purging __pycache__ and .pyc files...\")\ncache_dirs_removed = 0\npyc_files_removed  = 0\n\nfor cache_dir in SRC_ROOT.rglob(\"__pycache__\"):\n    shutil.rmtree(cache_dir)\n    cache_dirs_removed += 1\n\nfor pyc_file in SRC_ROOT.rglob(\"*.pyc\"):\n    pyc_file.unlink()\n    pyc_files_removed += 1\n\nprint(f\"  Removed {cache_dirs_removed} __pycache__ dirs, {pyc_files_removed} .pyc files\")\n\n# ── STEP 2: sys.modules Scrub ──────────────────────────────────────────────\nprint(\"\\n[2/4] Scrubbing src.* from sys.modules...\")\nstale_keys = [k for k in sys.modules if k.startswith(SCRUB_PREFIXES)]\nfor key in stale_keys:\n    del sys.modules[key]\nprint(f\"  Evicted {len(stale_keys)} cached modules: {stale_keys[:8]}{'...' if len(stale_keys) > 8 else ''}\")\n\n# ── STEP 3: Path Priority ─────────────────────────────────────────────────\nprint(\"\\n[3/4] Pinning sys.path priority...\")\nrepo_str = str(REPO_ROOT)\n# Remove any existing entries to avoid duplicates\nsys.path = [p for p in sys.path if p != repo_str]\n# Insert at position 0 so our src/ wins over any pip-installed version\nsys.path.insert(0, repo_str)\nprint(f\"  sys.path[0] = {sys.path[0]}\")\n\n# ── STEP 4: Source File Validation ─────────────────────────────────────────\nprint(\"\\n[4/4] Verifying source file integrity...\")\n\ndef file_fingerprint(path: Path) -> dict:\n    \"\"\"Return last-modified timestamp and MD5 hash for a file.\"\"\"\n    data = path.read_bytes()\n    md5  = hashlib.md5(data).hexdigest()\n    mtime = datetime.fromtimestamp(path.stat().st_mtime, tz=timezone.utc)\n    return {\"md5\": md5, \"modified\": mtime.strftime(\"%Y-%m-%d %H:%M:%S UTC\"), \"size\": len(data)}\n\nfor fpath in VERIFY_FILES:\n    if fpath.exists():\n        info = file_fingerprint(fpath)\n        rel  = fpath.relative_to(REPO_ROOT)\n        print(f\"  ✓ {rel}\")\n        print(f\"    Modified : {info['modified']}\")\n        print(f\"    MD5      : {info['md5']}\")\n        print(f\"    Size     : {info['size']:,} bytes\")\n    else:\n        print(f\"  ⚠ {fpath} — NOT FOUND (skipped)\")\n\n# ── STEP 5: Fresh Imports ──────────────────────────────────────────────────\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Importing modules (fresh)...\")\n\nfrom src.config import BenchmarkConfig\nfrom src.utils.seeds import set_seeds\nfrom src.utils.checkpoints import (\n    ensure_drive_mounted,\n    checkpoint_exists,\n    save_checkpoint,\n    load_checkpoint,\n    create_checkpoint_dirs,\n)\nfrom src.data.dataset_loader import RoGWebQSPLoader\nfrom src.data.graph_builder import GraphBuilder\n\nprint(\"✓ All imports successful (clean load)\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CLEAN ROOM COMPLETE\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbv4KpD12XzD"
   },
   "source": [
    "## Cell 3: Configuration\n",
    "\n",
    "Initialize benchmark configuration with hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbf7sF7A2XzE",
    "outputId": "efae91da-0ceb-431d-91bf-e432271a18b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "arcOS Benchmark Configuration\n",
      "============================================================\n",
      "Seed: 42 (deterministic=True)\n",
      "Dataset: rmanluo/RoG-webqsp\n",
      "Drive root: /content/drive/MyDrive/arcOS_benchmark\n",
      "Checkpoint dir: /content/drive/MyDrive/arcOS_benchmark/checkpoints\n",
      "Results dir: /content/drive/MyDrive/arcOS_benchmark/results\n",
      "\n",
      "--- Retrieval ---\n",
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Top-K entities: 15\n",
      "PCST budget: 50\n",
      "PCST local budget: 500\n",
      "PCST edge cost: 1.0\n",
      "PCST pruning: strong\n",
      "PCST base prize ratio: 1.5\n",
      "\n",
      "--- GNN ---\n",
      "Hidden dim: 256\n",
      "Num layers: 3\n",
      "Num heads: 4\n",
      "Pooling: attention\n",
      "\n",
      "--- LLM ---\n",
      "Model: anthropic/claude-3.5-sonnet\n",
      "Provider: openrouter\n",
      "Temperature: 0.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize configuration\n",
    "config = BenchmarkConfig(\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    drive_root=\"/content/drive/MyDrive/arcOS_benchmark\",\n",
    ")\n",
    "\n",
    "# Print configuration summary\n",
    "config.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrmwQcua2XzG"
   },
   "source": [
    "## Cell 4: Seed Initialization\n",
    "\n",
    "Set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuvkZMYQ2XzH",
    "outputId": "9d0a2e8e-e4a2-48d7-e59c-deae42bcaaa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seeds set to 42 (deterministic=True)\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds(config.seed, config.deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgEi3ieQ2XzH"
   },
   "source": [
    "## Cell 5: Google Drive Setup\n",
    "\n",
    "Mount Google Drive and create checkpoint/results directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjnHwHNG2XzI",
    "outputId": "f562ecf5-3f47-4e21-f4b2-f15035b69400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "✓ Google Drive mounted at /content/drive\n",
      "✓ Checkpoint directory: /content/drive/MyDrive/arcOS_benchmark/checkpoints\n",
      "✓ Results directory: /content/drive/MyDrive/arcOS_benchmark/results\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "drive_mounted = ensure_drive_mounted()\n",
    "\n",
    "if drive_mounted:\n",
    "    # Create checkpoint and results directories\n",
    "    create_checkpoint_dirs(config.checkpoint_dir, config.results_dir)\n",
    "else:\n",
    "    print(\"⚠ Warning: Drive not mounted. Checkpointing will not work.\")\n",
    "    print(\"  Continuing with local /content/ storage (temporary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_PN8Rjb2XzI"
   },
   "source": [
    "## Cell 6: Dataset Loading\n",
    "\n",
    "Load RoG-WebQSP dataset from HuggingFace with Drive caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "HKOysb1o2XzJ",
    "outputId": "c1726f96-dc6e-424a-a92a-5b3847d29ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HuggingFace cache directory: /content/drive/MyDrive/arcOS_benchmark/checkpoints/huggingface_cache\n",
      "Loading dataset from checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'rmanluo/RoG-webqsp' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'rmanluo/RoG-webqsp' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: Failed to load dataset from checkpoint due to missing files: [Errno 2] Failed to open local file '/root/.cache/huggingface/datasets/rmanluo___ro_g-webqsp/default/0.0.0/c0632533135a06f8c5d536b420deec5fcb5c58f3/ro_g-webqsp-train-00000-of-00002.arrow'. Detail: [errno 2] No such file or directory\n",
      "  Falling back to downloading dataset from HuggingFace...\n",
      "Downloading dataset from HuggingFace...\n",
      "Loading dataset: rmanluo/RoG-webqsp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2ebedc32c44f0c8e238b5ee499b6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/900 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42c3cbf791e4121931ee82faba83eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002-d810a36ed97bc2(…):   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf57273fedcb465080dce4779dd94b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002-e53244e71082a3(…):   0%|          | 0.00/155M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf267af8b27448186fda4ce6caf1715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001-6ee6adc5b(…):   0%|          | 0.00/24.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba6d307a2e74a03847d3f9a52d7c9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00002-9ee8d68f7d951e1(…):   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d47a9789cc64d86aa300060259c2611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00001-of-00002-773a7b8213e159f(…):   0%|          | 0.00/93.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c14d594a79497da97b0a1bbb9692e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5734a3f36440d19ee53e1bb665b7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b2a30237734944bedc12936ffa9838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded dataset with splits: ['train', 'validation', 'test']\n",
      "  - train: 2826 examples\n",
      "  - validation: 246 examples\n",
      "  - test: 1628 examples\n",
      "✓ Checkpoint saved: /content/drive/MyDrive/arcOS_benchmark/checkpoints/dataset.pkl (pickle)\n",
      "✓ Dataset downloaded and saved to checkpoint.\n",
      "\n",
      "============================================================\n",
      "Dataset Slicing\n",
      "============================================================\n",
      "Original sizes:\n",
      "  Train: 2826\n",
      "  Validation: 246\n",
      "  Test: 1628\n",
      "\n",
      "Sliced sizes:\n",
      "  Train: 900\n",
      "  Validation: 90\n",
      "  Test: 1628\n",
      "\n",
      "✓ Dataset sliced successfully\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Dataset Schema Inspection\n",
      "============================================================\n",
      "Inspecting first split: train\n",
      "\n",
      "Fields:\n",
      "  - id: Value('string')\n",
      "  - question: Value('string')\n",
      "  - answer: List(Value('string'))\n",
      "  - q_entity: List(Value('string'))\n",
      "  - a_entity: List(Value('string'))\n",
      "  - graph: List(List(Value('string')))\n",
      "  - choices: List(Value('null'))\n",
      "\n",
      "✓ All expected fields present\n",
      "\n",
      "Sample Examples (first 1):\n",
      "\n",
      "--- Example 0 ---\n",
      "ID: WebQTrn-0\n",
      "Question: what is the name of justin bieber brother\n",
      "Answer: ['Jaxon Bieber']\n",
      "Question Entity: ['Justin Bieber']\n",
      "Answer Entity: ['Jaxon Bieber']\n",
      "Choices: []\n",
      "Graph: 9088 triples\n",
      "  Sample triple: ['P!nk', 'freebase.valuenotation.is_reviewed', 'Gender']\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Dataset Statistics\n",
      "============================================================\n",
      "\n",
      "--- train ---\n",
      "Total examples: 900\n",
      "Graph size (triples):\n",
      "  - Average: 4251.4\n",
      "  - Min: 68\n",
      "  - Max: 10185\n",
      "\n",
      "--- validation ---\n",
      "Total examples: 90\n",
      "Graph size (triples):\n",
      "  - Average: 4121.7\n",
      "  - Min: 68\n",
      "  - Max: 9496\n",
      "\n",
      "--- test ---\n",
      "Total examples: 1628\n",
      "Graph size (triples):\n",
      "  - Average: 4309.3\n",
      "  - Min: 2\n",
      "  - Max: 10810\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Dataset Split Validation\n",
      "============================================================\n",
      "Train: 900 ✓\n",
      "Validation: 90 ✓\n",
      "Test: 1628 ✓\n",
      "\n",
      "✓ All splits have expected sizes\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset loader\n",
    "cache_dir = config.checkpoint_dir / \"huggingface_cache\"\n",
    "loader = RoGWebQSPLoader(cache_dir=cache_dir)\n",
    "\n",
    "# Check for cached dataset\n",
    "dataset_checkpoint_path = config.get_checkpoint_path(\"dataset.pkl\")\n",
    "\n",
    "dataset = None # Initialize dataset to None\n",
    "\n",
    "if checkpoint_exists(dataset_checkpoint_path):\n",
    "    print(\"Loading dataset from checkpoint...\")\n",
    "    try:\n",
    "        dataset = load_checkpoint(dataset_checkpoint_path, format=\"pickle\")\n",
    "        print(\"✓ Dataset loaded from checkpoint.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠ Warning: Failed to load dataset from checkpoint due to missing files: {e}\")\n",
    "        print(\"  Falling back to downloading dataset from HuggingFace...\")\n",
    "        # If loading fails, proceed to download\n",
    "        pass # dataset remains None, so the next block will execute\n",
    "\n",
    "if dataset is None: # If dataset was not loaded successfully or checkpoint didn't exist\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    dataset = loader.load(dataset_name=config.dataset_name)\n",
    "    save_checkpoint(dataset, dataset_checkpoint_path, format=\"pickle\")\n",
    "    print(\"✓ Dataset downloaded and saved to checkpoint.\")\n",
    "\n",
    "# Slice dataset to desired sizes\n",
    "dataset = loader.slice_dataset(\n",
    "    dataset,\n",
    "    train_size=900,\n",
    "    val_size=90,\n",
    "    test_size=None  # Keep all test examples\n",
    ")\n",
    "\n",
    "# Inspect dataset schema\n",
    "loader.inspect_schema(dataset, num_examples=1)\n",
    "\n",
    "# Compute statistics\n",
    "loader.compute_statistics(dataset)\n",
    "\n",
    "# Validate split counts (updated for sliced dataset)\n",
    "split_valid = loader.validate_split_counts(\n",
    "    dataset,\n",
    "    expected_train=900,\n",
    "    expected_val=90,\n",
    "    expected_test=1628,  # Keep original test size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tpm668x2XzJ"
   },
   "source": [
    "## Cell 7: Graph Construction\n",
    "\n",
    "Build NetworkX graphs from dataset triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "UBzceOT52XzJ",
    "outputId": "fe7f4170-7b15-4b64-b334-10df45175d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GraphBuilder initialized (directed=True)\n",
      "Loading unified graph from checkpoint...\n",
      "✓ Checkpoint loaded: /content/drive/MyDrive/arcOS_benchmark/checkpoints/unified_graph.pkl (pickle)\n",
      "\n",
      "============================================================\n",
      "Unified Training Graph Information\n",
      "============================================================\n",
      "Nodes: 543170\n",
      "Edges: 1553935\n",
      "Directed: True\n",
      "Density: 0.000005\n",
      "Weakly connected: True\n",
      "\n",
      "Relation Statistics:\n",
      "Unique relations: 4797\n",
      "Top 10 relations:\n",
      "  - common.topic.notable_types: 73649\n",
      "  - freebase.valuenotation.is_reviewed: 39352\n",
      "  - location.location.containedby: 34265\n",
      "  - common.topic.notable_for: 33116\n",
      "  - people.person.profession: 25576\n",
      "  - people.person.gender: 22162\n",
      "  - common.topic.article: 19550\n",
      "  - people.person.nationality: 19116\n",
      "  - common.topic.webpage: 18736\n",
      "  - common.webpage.topic: 18719\n",
      "\n",
      "Degree Statistics:\n",
      "Average in-degree: 2.86\n",
      "Average out-degree: 2.86\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Graph Size Validation\n",
      "============================================================\n",
      "Nodes: 543170 ✓\n",
      "Edges: 1553935 ✓\n",
      "\n",
      "✓ Graph meets size requirements\n",
      "============================================================\n",
      "\n",
      "Building sample per-example graph...\n",
      "\n",
      "============================================================\n",
      "Sample Per-Example Graph Information\n",
      "============================================================\n",
      "Nodes: 1723\n",
      "Edges: 8286\n",
      "Directed: True\n",
      "Density: 0.002793\n",
      "Weakly connected: True\n",
      "\n",
      "Relation Statistics:\n",
      "Unique relations: 320\n",
      "Top 10 relations:\n",
      "  - freebase.valuenotation.is_reviewed: 898\n",
      "  - broadcast.content.artist: 771\n",
      "  - music.artist.genre: 742\n",
      "  - broadcast.artist.content: 631\n",
      "  - people.person.profession: 614\n",
      "  - common.topic.notable_types: 395\n",
      "  - people.person.gender: 208\n",
      "  - people.person.nationality: 179\n",
      "  - broadcast.content.genre: 136\n",
      "  - common.topic.notable_for: 135\n",
      "\n",
      "Degree Statistics:\n",
      "Average in-degree: 4.81\n",
      "Average out-degree: 4.81\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize graph builder\n",
    "graph_builder = GraphBuilder(directed=config.graph_directed)\n",
    "\n",
    "# Check for cached unified graph\n",
    "unified_graph_path = config.get_checkpoint_path(\"unified_graph.pkl\")\n",
    "\n",
    "if checkpoint_exists(unified_graph_path):\n",
    "    print(\"Loading unified graph from checkpoint...\")\n",
    "    unified_graph = load_checkpoint(unified_graph_path, format=\"pickle\")\n",
    "else:\n",
    "    print(\"Building unified graph from training split...\")\n",
    "    unified_graph = graph_builder.build_unified_graph(dataset[\"train\"])\n",
    "    save_checkpoint(unified_graph, unified_graph_path, format=\"pickle\")\n",
    "\n",
    "# Print graph statistics\n",
    "graph_builder.print_graph_info(unified_graph, name=\"Unified Training Graph\")\n",
    "\n",
    "# Validate graph size\n",
    "graph_valid = graph_builder.validate_graph_size(\n",
    "    unified_graph,\n",
    "    min_nodes=config.unified_graph_min_nodes,\n",
    "    min_edges=config.unified_graph_min_edges,\n",
    ")\n",
    "\n",
    "# Build sample per-example graph for demonstration\n",
    "print(\"\\nBuilding sample per-example graph...\")\n",
    "sample_example = dataset[\"train\"][0]\n",
    "sample_graph = graph_builder.build_from_triples(\n",
    "    sample_example[\"graph\"],\n",
    "    graph_id=sample_example[\"id\"]\n",
    ")\n",
    "graph_builder.print_graph_info(sample_graph, name=\"Sample Per-Example Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pHieT482XzJ"
   },
   "source": [
    "## Cell 8: Phase 1 Validation\n",
    "\n",
    "Automated validation of all Phase 1 success criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEm6SVWK2XzK",
    "outputId": "3a4f0cd1-aaec-4e94-8118-be8d0aa20264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Phase 1 Success Criteria Validation\n",
      "============================================================\n",
      "✓ Checkpoint saved: /content/drive/MyDrive/arcOS_benchmark/checkpoints/test_roundtrip.pkl (pickle)\n",
      "✓ Checkpoint loaded: /content/drive/MyDrive/arcOS_benchmark/checkpoints/test_roundtrip.pkl (pickle)\n",
      "\n",
      "Validation Results:\n",
      "  ✓ GPU Available\n",
      "  ✓ All Imports Successful\n",
      "  ✓ Dataset Splits Valid\n",
      "  ✓ Unified Graph Size Valid\n",
      "  ✓ Checkpoint Round-Trip\n",
      "\n",
      "============================================================\n",
      "✓ PHASE 1 COMPLETE - All criteria passed!\n",
      "\n",
      "Ready to proceed to Phase 2: Retrieval Pipeline\n",
      "============================================================\n",
      "\n",
      "Phase 1 Summary:\n",
      "  Dataset: rmanluo/RoG-webqsp\n",
      "  Training examples: 900\n",
      "  Validation examples: 90\n",
      "  Test examples: 1628\n",
      "  Unified graph nodes: 543170\n",
      "  Unified graph edges: 1553935\n",
      "  Checkpoints saved to: /content/drive/MyDrive/arcOS_benchmark/checkpoints\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 1 Success Criteria Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect validation results\n",
    "validation_results = {\n",
    "    \"GPU Available\": torch.cuda.is_available(),\n",
    "    \"All Imports Successful\": True,  # If we got here, imports worked\n",
    "    \"Dataset Splits Valid\": split_valid,\n",
    "    \"Unified Graph Size Valid\": graph_valid,\n",
    "}\n",
    "\n",
    "# Test checkpoint round-trip\n",
    "test_checkpoint_path = config.get_checkpoint_path(\"test_roundtrip.pkl\")\n",
    "test_data = {\"test\": \"round-trip\", \"value\": 42}\n",
    "try:\n",
    "    save_checkpoint(test_data, test_checkpoint_path, format=\"pickle\")\n",
    "    loaded_data = load_checkpoint(test_checkpoint_path, format=\"pickle\")\n",
    "    checkpoint_roundtrip_ok = (loaded_data == test_data)\n",
    "    validation_results[\"Checkpoint Round-Trip\"] = checkpoint_roundtrip_ok\n",
    "except Exception as e:\n",
    "    print(f\"Checkpoint round-trip failed: {e}\")\n",
    "    validation_results[\"Checkpoint Round-Trip\"] = False\n",
    "\n",
    "# Print results\n",
    "print(\"\\nValidation Results:\")\n",
    "all_passed = True\n",
    "for criterion, passed in validation_results.items():\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"  {status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_passed:\n",
    "    print(\"✓ PHASE 1 COMPLETE - All criteria passed!\")\n",
    "    print(\"\\nReady to proceed to Phase 2: Retrieval Pipeline\")\n",
    "else:\n",
    "    print(\"✗ PHASE 1 INCOMPLETE - Some criteria failed\")\n",
    "    print(\"\\nPlease review failed criteria above\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nPhase 1 Summary:\")\n",
    "print(f\"  Dataset: {config.dataset_name}\")\n",
    "print(f\"  Training examples: {len(dataset['train'])}\")\n",
    "print(f\"  Validation examples: {len(dataset['validation'])}\")\n",
    "print(f\"  Test examples: {len(dataset['test'])}\")\n",
    "print(f\"  Unified graph nodes: {unified_graph.number_of_nodes()}\")\n",
    "print(f\"  Unified graph edges: {unified_graph.number_of_edges()}\")\n",
    "print(f\"  Checkpoints saved to: {config.checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN_Yhru52XzK"
   },
   "source": [
    "## Cell 9: Build Retrieval Pipeline\n",
    "\n",
    "Initialize retrieval components (embeddings, FAISS index, PCST solver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 515ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 1.03s\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpcst-fast\u001b[0m\u001b[2m==1.0.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybind11\u001b[0m\u001b[2m==3.0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install pcst-fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954,
     "referenced_widgets": [
      "3b6e2a07271c470682ed57cd9d4ec68c",
      "506ddfadd72c4909bb9a2114d4cbfffe",
      "041c254f9d784065ba72b6a1c896e2c3",
      "f3b2fcc49b6e499bb078f6cdd90a363f",
      "f59f4bccda2c44ae9d15c4747b581a20",
      "70b4723234ef4730b6e20a685ecf7a5a",
      "099dbc8612f94ba4933b3bdcc06d069b",
      "1979c36da93e4028bb631b21d6edd717",
      "c42782e59f3f42b6968ebd3b6cd26ba7",
      "51ea187fab3a4b57be40aef0109cd90f",
      "c683e6e0e61a45dfb892cc701c70eb48"
     ]
    },
    "id": "I48MWhc62XzL",
    "outputId": "a3da42c9-bd23-43d8-ac19-953603bb6553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 2: RETRIEVAL PIPELINE\n",
      "============================================================\n",
      "============================================================\n",
      "BUILDING RETRIEVAL PIPELINE\n",
      "============================================================\n",
      "\n",
      "[1/4] Initializing text embedder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c59da3ab9f435e93598012c6cb72d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff052a5c06f429893b1f43a761831d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac2fe9668654c10b257ae74ae6e9ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec86f2317e74535b84e6f0712ee3173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c9139a0a434965a1548c16b3ac3db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e1fd7ae8f645b1ad05a16931cb88dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8017106d3db74430a6634927694e8a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65be734dacd746a99dc1cbaa2dd7c0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf272966b25243d0aee19ff294d07269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7187b37faa4c7eabd3d87d45a7460d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ddcc00eba8440593d297649de484c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a62db57efc14ef08cb26ddb5201633d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  - Device: cuda\n",
      "  - Embedding dimension: 384\n",
      "\n",
      "[2/4] Loading/computing entity embeddings...\n",
      "Loading cached embeddings from entity_embeddings.pkl\n",
      "✓ Checkpoint loaded: /content/drive/MyDrive/arcOS_benchmark/checkpoints/entity_embeddings.pkl (pickle)\n",
      "✓ Loaded 543170 entity embeddings\n",
      "\n",
      "[3/4] Loading/computing relation embeddings...\n",
      "Loading cached relation embeddings from relation_embeddings.pkl\n",
      "✓ Checkpoint loaded: /content/drive/MyDrive/arcOS_benchmark/checkpoints/relation_embeddings.pkl (pickle)\n",
      "✓ Loaded 4797 relation embeddings\n",
      "\n",
      "[4/4] Loading/building FAISS index...\n",
      "Loading cached FAISS index from faiss_index.bin\n",
      "✓ Loaded FAISS index from /content/drive/MyDrive/arcOS_benchmark/checkpoints/faiss_index.bin\n",
      "  - 543170 entities indexed\n",
      "\n",
      "Initializing PCST solver...\n",
      "✓ PCST solver ready (cost: 1.0, budget: 50, local: 500, pruning: strong, base_prize_ratio: 1.5)\n",
      "\n",
      "============================================================\n",
      "✓ RETRIEVAL PIPELINE READY\n",
      "============================================================\n",
      "\n",
      "✓ Retrieval pipeline initialized\n",
      "  - Entity embeddings: 543170 entities\n",
      "  - Top-K: 15\n",
      "  - PCST budget: 50 nodes\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: RETRIEVAL PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from src.retrieval import Retriever\n",
    "\n",
    "# Build retriever (uses checkpoints if available)\n",
    "retriever = Retriever.build_from_checkpoint_or_new(\n",
    "    config=config,\n",
    "    unified_graph=unified_graph  # From Phase 1 Cell 7\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Retrieval pipeline initialized\")\n",
    "print(f\"  - Entity embeddings: {len(retriever.entity_index)} entities\")\n",
    "print(f\"  - Top-K: {config.top_k_entities}\")\n",
    "print(f\"  - PCST budget: {config.pcst_budget} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VwowPg62XzL"
   },
   "source": [
    "## Cell 10: Retrieval Validation\n",
    "\n",
    "Test retrieval pipeline on 10 validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "HfRB6T_p2XzL",
    "outputId": "e92539b3-a96d-4956-f2f9-dc61faaa5fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RETRIEVAL VALIDATION (10 examples)\n",
      "============================================================\n",
      "  Localized: 500 nodes, 1876 edges\n",
      "  PCST input: 500 nodes, 1536 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 111 high-prize nodes, root=Sacha Baron Cohen...\n",
      "  PCST output: 489 nodes, 488 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 145 edges, connected=True\n",
      "\n",
      "[1/10] Q: how old is sacha baron cohen...\n",
      "  Topic entities (q_entity): ['Sacha Baron Cohen']\n",
      "  Answer entities: ['1971-10-13']\n",
      "  Seeds used: ['Sacha Baron Cohen', \"'Bruno' sneak peek at SXSW: Scoop on Sacha Baron Cohen's new comedy!\", \"Man portrayed as 'terrorist' from 'Bruno' sues Sacha Baron Cohen\", 'Benjamin Cohen', 'Lizabeth Cohen']...\n",
      "  Subgraph: 50 nodes, 145 edges\n",
      "  Hit: ✗\n",
      "  Time: 10127.4ms\n",
      "  Localized: 500 nodes, 1164 edges\n",
      "  PCST input: 500 nodes, 831 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 109 high-prize nodes, root=Cleveland...\n",
      "  PCST output: 498 nodes, 497 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 93 edges, connected=True\n",
      "\n",
      "[2/10] Q: what time zone am i in cleveland ohio...\n",
      "  Topic entities (q_entity): ['Cleveland']\n",
      "  Answer entities: ['Eastern Time Zone']\n",
      "  Seeds used: ['Cleveland', 'Ohio City', 'Time zone', 'Cleveland County', 'Downtown Cleveland']...\n",
      "  Subgraph: 50 nodes, 93 edges\n",
      "  Hit: ✗\n",
      "  Time: 131.8ms\n",
      "  Localized: 500 nodes, 1750 edges\n",
      "  PCST input: 500 nodes, 1388 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 80 high-prize nodes, root=Nina Dobrev...\n",
      "  PCST output: 492 nodes, 491 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 94 edges, connected=True\n",
      "\n",
      "[3/10] Q: what is nina dobrev nationality...\n",
      "  Topic entities (q_entity): ['Nina Dobrev']\n",
      "  Answer entities: ['Bulgaria', 'Canada']\n",
      "  Seeds used: ['Nina Dobrev', 'Nina Dobrev Pictures', \"Nina Dobrev of 'Vampire Diaries' on getting advice from the 'True Blood' cast\", 'Nina Cassady', 'Nina']...\n",
      "  Subgraph: 50 nodes, 94 edges\n",
      "  Hit: ✓\n",
      "  Time: 112.7ms\n",
      "  Localized: 500 nodes, 1392 edges\n",
      "  PCST input: 500 nodes, 884 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 49 high-prize nodes, root=London Heathrow Airport...\n",
      "  PCST output: 495 nodes, 494 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 154 edges, connected=True\n",
      "\n",
      "[4/10] Q: what county is heathrow airport in...\n",
      "  Topic entities (q_entity): ['London Heathrow Airport']\n",
      "  Answer entities: ['Hillingdon']\n",
      "  Seeds used: ['London Heathrow Airport', 'Heathrow Airport Holdings', 'Chautauqua County-Jamestown Airport', 'Porter County Regional Airport', 'Monroe County Airport']...\n",
      "  Subgraph: 50 nodes, 154 edges\n",
      "  Hit: ✗\n",
      "  Time: 107.5ms\n",
      "  Localized: 500 nodes, 1143 edges\n",
      "  PCST input: 500 nodes, 861 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 41 high-prize nodes, root=Sparta...\n",
      "  PCST output: 498 nodes, 497 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 137 edges, connected=True\n",
      "\n",
      "[5/10] Q: what form of government was practiced in sparta...\n",
      "  Topic entities (q_entity): ['Sparta']\n",
      "  Answer entities: ['Monarchy', 'Aristocracy', 'Diarchy', 'Republic']\n",
      "  Seeds used: ['Sparta', 'Sparta should be contained in some Greek prefecture.', 'Hellenistic and Roman Sparta', 'Agesilaos and the crisis of Sparta', 'Sparta Township']...\n",
      "  Subgraph: 50 nodes, 137 edges\n",
      "  Hit: ✗\n",
      "  Time: 108.8ms\n",
      "  Localized: 500 nodes, 1487 edges\n",
      "  PCST input: 500 nodes, 890 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 104 high-prize nodes, root=John F. Kennedy...\n",
      "  PCST output: 500 nodes, 499 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 109 edges, connected=True\n",
      "\n",
      "[6/10] Q: who was the president after jfk died...\n",
      "  Topic entities (q_entity): ['John F. Kennedy']\n",
      "  Answer entities: ['Lyndon B. Johnson']\n",
      "  Seeds used: ['John F. Kennedy', 'John F. Kennedy Jr.', 'Reclaiming History: The Assassination of President John F. Kennedy', 'An Unfinished Life: John F. Kennedy, 1917–1963', '50th Anniversary of the Assassination of John F. Kennedy']...\n",
      "  Subgraph: 50 nodes, 109 edges\n",
      "  Hit: ✗\n",
      "  Time: 103.9ms\n",
      "  Localized: 500 nodes, 1367 edges\n",
      "  PCST input: 500 nodes, 1164 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 42 high-prize nodes, root=Bangkok...\n",
      "  PCST output: 500 nodes, 499 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 116 edges, connected=True\n",
      "\n",
      "[7/10] Q: where to visit near bangkok...\n",
      "  Topic entities (q_entity): ['Bangkok']\n",
      "  Answer entities: ['Bangkok International Trade and Exhibition Centre', 'Dusit Zoo', 'Wat Arun', 'Magic Land', 'Wat Ratchanatdaram', 'Erawan Shrine', 'Wat Pho', 'Khaosan Road', 'Wat Saket', 'Grand Palace', 'Wat Benchamabophit', 'Golden Buddha', 'Vimanmek Mansion', 'Samutprakarn Crocodile Farm and Zoo', 'Chatuchak Park', 'Ananta Samakhom Throne Hall', 'Democracy Monument', 'Thonburi', 'Rajamangala Stadium', 'Wat Suthat', 'Jim Thompson House', 'Bangkok Aquarium', 'Siam Park City', 'Safari World', 'Lumphini Park', 'Wat Phra Kaew', 'Bangkok National Museum']\n",
      "  Seeds used: ['Bangkok', 'The Red Tour, Bangkok', 'Bangkok Babylon', 'Assignment Bangkok', 'Bangkok Post']...\n",
      "  Subgraph: 50 nodes, 116 edges\n",
      "  Hit: ✗\n",
      "  Time: 108.8ms\n",
      "  Localized: 500 nodes, 1886 edges\n",
      "  PCST input: 500 nodes, 1748 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 58 high-prize nodes, root=Mick Clegg...\n",
      "  PCST output: 464 nodes, 463 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 199 edges, connected=True\n",
      "\n",
      "[8/10] Q: what did nick clegg study at university...\n",
      "  Topic entities (q_entity): ['Nick Clegg']\n",
      "  Answer entities: ['Social anthropology', 'Political philosophy']\n",
      "  Seeds used: ['Mick Clegg', 'george julius poulett scrope taught by adam sedgwick', 'Nick Takes Over School', 'Claudine Gay', 'Clarence Clemons & Jackson Browne']...\n",
      "  Subgraph: 50 nodes, 199 edges\n",
      "  Hit: ✗\n",
      "  Time: 109.2ms\n",
      "  Localized: 500 nodes, 1419 edges\n",
      "  PCST input: 500 nodes, 961 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 67 high-prize nodes, root=Henry Clay...\n",
      "  PCST output: 494 nodes, 493 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 186 edges, connected=True\n",
      "\n",
      "[9/10] Q: what is henry clay known for...\n",
      "  Topic entities (q_entity): ['Henry Clay']\n",
      "  Answer entities: ['Lawyer', 'Politician', 'Statesman']\n",
      "  Seeds used: ['Henry Clay', 'Lewis Clay', 'Henry Clay Frick', \"Henry Clay's Law Office\", 'Henry Clay Ide']...\n",
      "  Subgraph: 50 nodes, 186 edges\n",
      "  Hit: ✗\n",
      "  Time: 105.4ms\n",
      "  Localized: 500 nodes, 1513 edges\n",
      "  PCST input: 500 nodes, 918 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 136 high-prize nodes, root=John F. Kennedy...\n",
      "  PCST output: 500 nodes, 499 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 109 edges, connected=True\n",
      "\n",
      "[10/10] Q: where was kennedy when he got shot...\n",
      "  Topic entities (q_entity): ['John F. Kennedy']\n",
      "  Answer entities: ['Dallas']\n",
      "  Seeds used: ['John F. Kennedy', '50th Anniversary of the Assassination of John F. Kennedy', \"The circumstances of Kennedy's death led to a multitude of theories about who was responsible for his death.\", 'He was also seen on the first floor of the depository about 10 minutes before President Kennedy was shot as he drove in an open car at 12.30 pm.', 'Mortal Error: The Shot That Killed JFK']...\n",
      "  Subgraph: 50 nodes, 109 edges\n",
      "  Hit: ✗\n",
      "  Time: 110.1ms\n",
      "\n",
      "============================================================\n",
      "VALIDATION SUMMARY\n",
      "============================================================\n",
      "Hit rate: 10.0% (1/10)\n",
      "Avg retrieval time: 1112.6ms\n",
      "Avg subgraph size: 50.0 nodes\n",
      "Max subgraph size: 50 nodes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RETRIEVAL VALIDATION (10 examples)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use first 10 validation examples\n",
    "val_examples = list(dataset[\"validation\"].select(range(10)))\n",
    "\n",
    "hit_count = 0\n",
    "total_time_ms = 0\n",
    "subgraph_sizes = []\n",
    "\n",
    "for i, example in enumerate(val_examples):\n",
    "    question = example[\"question\"]\n",
    "    answer_entities = example.get(\"a_entity\", [])\n",
    "    if isinstance(answer_entities, str):\n",
    "        answer_entities = [answer_entities]\n",
    "\n",
    "    # Extract topic entities from dataset\n",
    "    q_entities = example.get(\"q_entity\", [])\n",
    "    if isinstance(q_entities, str):\n",
    "        q_entities = [q_entities]\n",
    "\n",
    "    # Retrieve subgraph (q_entity used as primary seed when available)\n",
    "    result = retriever.retrieve(question, q_entity=q_entities)\n",
    "\n",
    "    # Check if answer entity in subgraph\n",
    "    subgraph_nodes = set(result.subgraph.nodes())\n",
    "    hit = any(ans in subgraph_nodes for ans in answer_entities)\n",
    "\n",
    "    if hit:\n",
    "        hit_count += 1\n",
    "\n",
    "    total_time_ms += result.retrieval_time_ms\n",
    "    subgraph_sizes.append(result.num_nodes)\n",
    "\n",
    "    # Print example\n",
    "    print(f\"\\n[{i+1}/10] Q: {question[:60]}...\")\n",
    "    print(f\"  Topic entities (q_entity): {q_entities}\")\n",
    "    print(f\"  Answer entities: {answer_entities}\")\n",
    "    print(f\"  Seeds used: {result.seed_entities[:5]}{'...' if len(result.seed_entities) > 5 else ''}\")\n",
    "    print(f\"  Subgraph: {result.num_nodes} nodes, {result.num_edges} edges\")\n",
    "    print(f\"  Hit: {'✓' if hit else '✗'}\")\n",
    "    print(f\"  Time: {result.retrieval_time_ms:.1f}ms\")\n",
    "\n",
    "# Summary metrics\n",
    "hit_rate = hit_count / len(val_examples) * 100\n",
    "avg_time = total_time_ms / len(val_examples)\n",
    "avg_size = sum(subgraph_sizes) / len(subgraph_sizes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Hit rate: {hit_rate:.1f}% ({hit_count}/{len(val_examples)})\")\n",
    "print(f\"Avg retrieval time: {avg_time:.1f}ms\")\n",
    "print(f\"Avg subgraph size: {avg_size:.1f} nodes\")\n",
    "print(f\"Max subgraph size: {max(subgraph_sizes)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju6e-sE22XzM"
   },
   "source": [
    "## Cell 11: Phase 2 Success Criteria\n",
    "\n",
    "Validate Phase 2 completion criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEDJ25h_2XzM",
    "outputId": "57355918-2aa3-4a8e-a4ef-5a0bfd4f4020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 2 SUCCESS CRITERIA\n",
      "============================================================\n",
      "[✗] Retrieval completes in <1 second: 1112.6ms\n",
      "[✗] Subgraph contains answer entity >60%: 10.0%\n",
      "  Localized: 500 nodes, 1876 edges\n",
      "  PCST input: 500 nodes, 1536 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 111 high-prize nodes, root=Sacha Baron Cohen...\n",
      "  PCST output: 489 nodes, 488 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 145 edges, connected=True\n",
      "  Localized: 500 nodes, 1164 edges\n",
      "  PCST input: 500 nodes, 831 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 109 high-prize nodes, root=Cleveland...\n",
      "  PCST output: 498 nodes, 497 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 93 edges, connected=True\n",
      "  Localized: 500 nodes, 1750 edges\n",
      "  PCST input: 500 nodes, 1388 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 80 high-prize nodes, root=Nina Dobrev...\n",
      "  PCST output: 492 nodes, 491 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 94 edges, connected=True\n",
      "  Localized: 500 nodes, 1392 edges\n",
      "  PCST input: 500 nodes, 884 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 49 high-prize nodes, root=London Heathrow Airport...\n",
      "  PCST output: 495 nodes, 494 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 154 edges, connected=True\n",
      "  Localized: 500 nodes, 1143 edges\n",
      "  PCST input: 500 nodes, 861 edges, cost=1.00, base_prize=1.50, seed_floor=5.0, pruning='strong', 41 high-prize nodes, root=Sparta...\n",
      "  PCST output: 498 nodes, 497 edges\n",
      "  PCST result too small (1 nodes), falling back to BFS\n",
      "  Final: 50 nodes, 137 edges, connected=True\n",
      "[✓] All subgraphs are connected\n",
      "[✓] Subgraph size ≤ budget (50 ≤ 50)\n",
      "\n",
      "============================================================\n",
      "⚠ PHASE 2 INCOMPLETE - Review failed criteria above\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2 SUCCESS CRITERIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criterion 1: Retrieval speed < 1 second\n",
    "speed_pass = avg_time < 1000  # ms\n",
    "print(f\"[{'✓' if speed_pass else '✗'}] Retrieval completes in <1 second: {avg_time:.1f}ms\")\n",
    "\n",
    "# Criterion 2: Hit rate > 60%\n",
    "hit_pass = hit_rate >= 60.0\n",
    "print(f\"[{'✓' if hit_pass else '✗'}] Subgraph contains answer entity >60%: {hit_rate:.1f}%\")\n",
    "\n",
    "# Criterion 3: All subgraphs connected\n",
    "all_connected = all(\n",
    "    nx.is_weakly_connected(\n",
    "        retriever.retrieve(\n",
    "            example[\"question\"],\n",
    "            q_entity=example.get(\"q_entity\")\n",
    "        ).subgraph\n",
    "    )\n",
    "    for example in val_examples[:5]  # Check first 5\n",
    ")\n",
    "print(f\"[{'✓' if all_connected else '✗'}] All subgraphs are connected\")\n",
    "\n",
    "# Criterion 4: Subgraph size respects budget\n",
    "size_pass = max(subgraph_sizes) <= config.pcst_budget\n",
    "print(f\"[{'✓' if size_pass else '✗'}] Subgraph size ≤ budget ({max(subgraph_sizes)} ≤ {config.pcst_budget})\")\n",
    "\n",
    "# Overall pass\n",
    "all_pass = speed_pass and hit_pass and all_connected and size_pass\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_pass:\n",
    "    print(\"✓ PHASE 2 COMPLETE - All criteria met!\")\n",
    "    print(\"\\nReady to proceed to Phase 3: GNN Encoder\")\n",
    "else:\n",
    "    print(\"⚠ PHASE 2 INCOMPLETE - Review failed criteria above\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdPaAEYiab3A"
   },
   "source": [
    "# Phase 3: GNN Encoder\n",
    "## Cell 12: Build/Load GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8YgdaqamjdWl",
    "outputId": "15f3fbd3-01fd-45be-dff2-21fef43fcace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m29 packages\u001b[0m \u001b[2min 115ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 41.51s\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m5 packages\u001b[0m \u001b[2min 698ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 201ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0+cu128\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.9.0+cu128\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.0+cu128\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.23.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbTg--JZkzUx",
    "outputId": "42a8b6fa-888d-4dc5-bc5a-664231ac08d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing torch_geometric...\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m26 packages\u001b[0m \u001b[2min 762ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 493ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 18ms\u001b[0m\u001b[0m0                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch-geometric\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch-scatter\u001b[0m\u001b[2m==2.1.2+pt28cu128\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch-sparse\u001b[0m\u001b[2m==0.6.18+pt28cu128\u001b[0m\n",
      "✓ torch_geometric installed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Install torch_geometric and its dependencies\n",
    "print(\"Installing torch_geometric...\")\n",
    "!uv pip install torch_geometric torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.8.0+cu128.html\n",
    "print(\"✓ torch_geometric installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "kiad8bLiab3B",
    "outputId": "c1a86141-eb79-49d1-8de3-c4354834b1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building GNN Model...\n",
      "This will either:\n",
      "  1. Load pre-trained model from checkpoint, OR\n",
      "  2. Prepare data and train from scratch (~30 min)\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2760833626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGNNModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Build GNN model (handles checkpoint loading or training automatically)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3: GNN Encoder\n",
    "# ============================================================\n",
    "\n",
    "print(\"Building GNN Model...\")\n",
    "print(\"This will either:\")\n",
    "print(\"  1. Load pre-trained model from checkpoint, OR\")\n",
    "print(\"  2. Prepare data and train from scratch (~30 min)\")\n",
    "print()\n",
    "\n",
    "from src.gnn import GNNModel\n",
    "\n",
    "# Build GNN model (handles checkpoint loading or training automatically)\n",
    "gnn_model = GNNModel.build_from_checkpoint_or_train(\n",
    "    config=config,\n",
    "    retriever=retriever,\n",
    "    train_data=dataset[\"train\"],\n",
    "    val_data=dataset[\"validation\"],\n",
    "    encoder_type=\"gatv2\",  # or \"graphsage\"\n",
    "    pooling_type=\"attention\",  # or \"mean\", \"max\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GNN Model Ready\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BbYDV0Zab3B"
   },
   "source": [
    "## Cell 13: Test GNN Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "J90pfuVTab3B",
    "outputId": "1d2ed4d0-9f08-4c45-a96c-07b57a64d634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GNN inference on example query...\n",
      "\n",
      "Question: Who is Justin Bieber's brother?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-45622549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Retrieve subgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mretrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Retrieved subgraph: {retrieved.num_nodes} nodes, {retrieved.num_edges} edges\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# Test GNN encoding on a single example\n",
    "print(\"Testing GNN inference on example query...\\n\")\n",
    "\n",
    "test_question = \"Who is Justin Bieber's brother?\"\n",
    "print(f\"Question: {test_question}\")\n",
    "\n",
    "# Retrieve subgraph\n",
    "retrieved = retriever.retrieve(test_question)\n",
    "print(f\"Retrieved subgraph: {retrieved.num_nodes} nodes, {retrieved.num_edges} edges\")\n",
    "\n",
    "# Encode with GNN\n",
    "gnn_output = gnn_model.encode(retrieved, test_question)\n",
    "\n",
    "print(f\"\\nGNN Output:\")\n",
    "print(f\"  Node embeddings shape: {gnn_output.node_embeddings.shape}\")\n",
    "print(f\"  Graph embedding shape: {gnn_output.graph_embedding.shape}\")\n",
    "print(f\"  Attention scores: {len(gnn_output.attention_scores)} nodes\")\n",
    "\n",
    "# Get top attention nodes\n",
    "top_nodes = gnn_model.get_top_attention_nodes(gnn_output, top_k=10)\n",
    "print(f\"\\nTop 10 nodes by attention score:\")\n",
    "for i, (node, score) in enumerate(top_nodes, 1):\n",
    "    print(f\"  {i}. {node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYEaOgU3ab3B"
   },
   "source": [
    "## Cell 14: Validate GNN Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lXQ_Uzgab3B"
   },
   "outputs": [],
   "source": [
    "# Load training history and display metrics\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_path = config.get_checkpoint_path(\"gnn_training_history.json\")\n",
    "with open(history_path, \"r\") as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 3 VALIDATION: GNN Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best metrics\n",
    "best_val_f1 = max(history[\"val_f1\"])\n",
    "best_val_loss = min(history[\"val_loss\"])\n",
    "final_val_f1 = history[\"val_f1\"][-1]\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Epochs trained: {len(history['train_loss'])}\")\n",
    "print(f\"  Best validation F1: {best_val_f1:.3f}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Final validation F1: {final_val_f1:.3f}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training and Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# F1 curve\n",
    "axes[1].plot(history[\"train_f1\"], label=\"Train F1\")\n",
    "axes[1].plot(history[\"val_f1\"], label=\"Val F1\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"F1 Score\")\n",
    "axes[1].set_title(\"Answer Node Prediction F1\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Success criteria check\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Success Criteria:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "criteria = [\n",
    "    (\"Validation F1 > 0.5\", best_val_f1 > 0.5, best_val_f1),\n",
    "    (\"Training completed < 30 min\", True, \"N/A\"),  # User observation\n",
    "    (\"No OOM errors\", True, \"N/A\"),  # User observation\n",
    "]\n",
    "\n",
    "for criterion, passed, value in criteria:\n",
    "    status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "    print(f\"{status} - {criterion}: {value}\")\n",
    "\n",
    "if all(c[1] for c in criteria):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUCCESS: Phase 3 Complete\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FAILED: Some criteria not met\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V278nVPzab3B"
   },
   "source": [
    "## Cell 15: Visualize Attention on Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0RRpTpHab3B"
   },
   "outputs": [],
   "source": [
    "# Visualize GNN attention on a subgraph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def visualize_gnn_attention(\n",
    "    subgraph: nx.DiGraph,\n",
    "    attention_scores: dict,\n",
    "    answer_entities: list,\n",
    "    question: str,\n",
    "    top_k: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize GNN attention on a subgraph.\n",
    "\n",
    "    Args:\n",
    "        subgraph: NetworkX DiGraph\n",
    "        attention_scores: Dict[node_name, float]\n",
    "        answer_entities: List of ground truth answer entities\n",
    "        question: Question text\n",
    "        top_k: Show only top-K nodes by attention\n",
    "    \"\"\"\n",
    "    # Get top-K nodes by attention\n",
    "    sorted_nodes = sorted(\n",
    "        attention_scores.items(), key=lambda x: x[1], reverse=True\n",
    "    )[:top_k]\n",
    "    top_nodes = [node for node, _ in sorted_nodes]\n",
    "\n",
    "    # Create subgraph with only top nodes\n",
    "    G_viz = subgraph.subgraph(top_nodes).copy()\n",
    "\n",
    "    # Node colors (red = answer, blue = high attention, gray = low attention)\n",
    "    node_colors = []\n",
    "    for node in G_viz.nodes():\n",
    "        if node in answer_entities:\n",
    "            node_colors.append(\"red\")\n",
    "        else:\n",
    "            # Scale by attention (darker = higher attention)\n",
    "            attn = attention_scores.get(node, 0.0)\n",
    "            intensity = min(attn * 10, 1.0)  # Scale for visibility\n",
    "            node_colors.append((0.2, 0.4, 0.8, 0.3 + 0.7 * intensity))\n",
    "\n",
    "    # Node sizes proportional to attention\n",
    "    node_sizes = [\n",
    "        300 + 2000 * attention_scores.get(node, 0.0) for node in G_viz.nodes()\n",
    "    ]\n",
    "\n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G_viz, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.title(f\"GNN Attention Visualization\\nQ: {question}\", fontsize=12)\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(\n",
    "        G_viz, pos, alpha=0.3, arrows=True, arrowsize=10, width=1.0\n",
    "    )\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(\n",
    "        G_viz, pos, node_color=node_colors, node_size=node_sizes, alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Draw labels (only for top 10)\n",
    "    labels = {node: node[:20] for node in list(G_viz.nodes())[:10]}\n",
    "    nx.draw_networkx_labels(G_viz, pos, labels, font_size=8)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print attention scores\n",
    "    print(\"Top 10 attention scores:\")\n",
    "    for i, (node, score) in enumerate(sorted_nodes[:10], 1):\n",
    "        is_answer = \"✓ ANSWER\" if node in answer_entities else \"\"\n",
    "        print(f\"  {i}. {node[:30]}: {score:.4f} {is_answer}\")\n",
    "\n",
    "\n",
    "# Test visualization\n",
    "test_question = \"Who is Barack Obama's spouse?\"\n",
    "test_answer = [\"Michelle Obama\", \"m.025s5v9\"]  # Freebase ID\n",
    "\n",
    "retrieved = retriever.retrieve(test_question)\n",
    "gnn_output = gnn_model.encode(retrieved, test_question)\n",
    "\n",
    "visualize_gnn_attention(\n",
    "    subgraph=retrieved.subgraph,\n",
    "    attention_scores=gnn_output.attention_scores,\n",
    "    answer_entities=test_answer,\n",
    "    question=test_question,\n",
    "    top_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uV1qeDzzab3C"
   },
   "source": [
    "## Cell 16: Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URJaPvgLab3C",
    "outputId": "b578e4b6-917f-4933-bafe-0357083787bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Summary:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"  Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    # Verify no memory leak\n",
    "    assert torch.cuda.memory_allocated() / 1e9 < 14.0, \"Memory leak detected!\"\n",
    "    print(\"\\n✓ Memory usage within acceptable range\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "041c254f9d784065ba72b6a1c896e2c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1979c36da93e4028bb631b21d6edd717",
      "max": 103,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c42782e59f3f42b6968ebd3b6cd26ba7",
      "value": 103
     }
    },
    "099dbc8612f94ba4933b3bdcc06d069b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1979c36da93e4028bb631b21d6edd717": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b6e2a07271c470682ed57cd9d4ec68c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_506ddfadd72c4909bb9a2114d4cbfffe",
       "IPY_MODEL_041c254f9d784065ba72b6a1c896e2c3",
       "IPY_MODEL_f3b2fcc49b6e499bb078f6cdd90a363f"
      ],
      "layout": "IPY_MODEL_f59f4bccda2c44ae9d15c4747b581a20"
     }
    },
    "506ddfadd72c4909bb9a2114d4cbfffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70b4723234ef4730b6e20a685ecf7a5a",
      "placeholder": "​",
      "style": "IPY_MODEL_099dbc8612f94ba4933b3bdcc06d069b",
      "value": "Loading weights: 100%"
     }
    },
    "51ea187fab3a4b57be40aef0109cd90f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70b4723234ef4730b6e20a685ecf7a5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c42782e59f3f42b6968ebd3b6cd26ba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c683e6e0e61a45dfb892cc701c70eb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3b2fcc49b6e499bb078f6cdd90a363f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51ea187fab3a4b57be40aef0109cd90f",
      "placeholder": "​",
      "style": "IPY_MODEL_c683e6e0e61a45dfb892cc701c70eb48",
      "value": " 103/103 [00:00&lt;00:00, 315.76it/s, Materializing param=pooler.dense.weight]"
     }
    },
    "f59f4bccda2c44ae9d15c4747b581a20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}